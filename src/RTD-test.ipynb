{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ddc17ed5-c7ab-4338-884a-cbe2ede27a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d672d9ad7c2f4df986ec4908f2056159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/668 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd16fdab15be4031851fa79fcf089d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForPreTraining: ['electra.embeddings_project.bias', 'electra.embeddings_project.weight']\n",
      "- This IS expected if you are initializing ElectraForPreTraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForPreTraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3628e371f34a52a9a29c1533c43beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99ecf8483fe48d4b696682b7b3d3dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72fcce3484444a18eb4cf110f03f890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b28422bf8ce47c0899a29f4282648c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import ElectraForPreTraining, ElectraTokenizerFast\n",
    "import torch\n",
    "\n",
    "discriminator = ElectraForPreTraining.from_pretrained(\"google/electra-large-discriminator\")\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-large-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d7ac1ee-4a30-4e5f-b7ca-87e9c95307a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fake_tokens(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence, add_special_tokens=True)\n",
    "    embed = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    discriminator_outputs = discriminator(embed)\n",
    "    # Shift logits to binary labels\n",
    "    predictions = torch.round((torch.sign(discriminator_outputs.logits[0]) + 1) / 2)\n",
    "    \n",
    "    for token, prediction in zip(tokens, predictions.tolist()):\n",
    "        print(f\"{token:>10} : {int(prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f80848db-d0f1-4f2d-8dfa-2f84b8c9e463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [CLS] : 0\n",
      "       the : 0\n",
      " scientist : 0\n",
      " explained : 0\n",
      "      them : 1\n",
      "       the : 0\n",
      "experiment : 0\n",
      "    worked : 0\n",
      "     [SEP] : 0\n",
      "================================================================================\n",
      "     [CLS] : 0\n",
      "   despite : 0\n",
      "       the : 0\n",
      "   weather : 0\n",
      "         , : 0\n",
      "     enjoy : 1\n",
      "   enjoyed : 0\n",
      "       the : 0\n",
      "    picnic : 0\n",
      "     [SEP] : 0\n",
      "================================================================================\n",
      "     [CLS] : 0\n",
      "       the : 0\n",
      "   student : 0\n",
      "       who : 0\n",
      "      make : 1\n",
      " yesterday : 0\n",
      "    passed : 0\n",
      "       the : 0\n",
      "      test : 0\n",
      "     [SEP] : 0\n",
      "================================================================================\n",
      "     [CLS] : 0\n",
      "       she : 0\n",
      "  believes : 0\n",
      "      that : 0\n",
      "        is : 0\n",
      "       the : 0\n",
      "    belief : 1\n",
      "    answer : 1\n",
      "     [SEP] : 0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "fake_sentence = \"The quick brown fox jumps over fake lazy dog\"\n",
    "\n",
    "sentences = [\n",
    "    \"The scientist explained THEM the experiment worked\",\n",
    "    \"Despite the weather, ENJOY enjoyed the picnic\",\n",
    "    \"The student who MAKE yesterday passed the test\",\n",
    "    \"She believes that is the BELIEF answer\",\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print_fake_tokens(sentence)\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8a0fffc-e48f-4896-8cbb-459e86369997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top candidates for cloze gaps:\n",
      "            the: 4.101\n",
      "            the: 2.042\n",
      "          stood: 2.003\n",
      "         remain: 1.945\n",
      "          these: 1.484\n"
     ]
    }
   ],
   "source": [
    "def get_token_scores(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence, add_special_tokens=True)\n",
    "    embed = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    outputs = discriminator(embed)\n",
    "    # Keep raw logits instead of binary predictions for more granular scoring\n",
    "    scores = outputs.logits[0].tolist()\n",
    "    return list(zip(tokens, scores))\n",
    "\n",
    "def analyze_context_windows(text, window_size=10):\n",
    "    # Get global scores\n",
    "    global_scores = get_token_scores(text)\n",
    "    \n",
    "    # Get local scores by sliding window\n",
    "    words = text.split()\n",
    "    local_scores_by_token = {}\n",
    "    \n",
    "    for i in range(len(words) - window_size + 1):\n",
    "        window = \" \".join(words[i:i + window_size])\n",
    "        local_scores = get_token_scores(window)\n",
    "        \n",
    "        # Store scores for each token appearance\n",
    "        for token, score in local_scores[1:-1]:  # Skip special tokens\n",
    "            if token not in local_scores_by_token:\n",
    "                local_scores_by_token[token] = []\n",
    "            local_scores_by_token[token].append(score)\n",
    "    \n",
    "    # Find interesting tokens (high global score, low local scores)\n",
    "    interesting_tokens = []\n",
    "    for token, global_score in global_scores[1:-1]:  # Skip special tokens\n",
    "        if token in local_scores_by_token:\n",
    "            avg_local_score = sum(local_scores_by_token[token]) / len(local_scores_by_token[token])\n",
    "            score_diff = global_score - avg_local_score\n",
    "            interesting_tokens.append((token, score_diff))\n",
    "    \n",
    "    # Sort by score difference\n",
    "    return sorted(interesting_tokens, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Test with a sample passage\n",
    "text = \"\"\"The ancient castle stood atop the hill, its weathered stones telling stories of centuries past. \n",
    "Knights once roamed these halls, their armor gleaming in the torchlight. Now only whispers remain, \n",
    "echoing through the empty corridors.\"\"\"\n",
    "\n",
    "# Get potential cloze tokens\n",
    "candidates = analyze_context_windows(text)\n",
    "print(\"Top candidates for cloze gaps:\")\n",
    "for token, score_diff in candidates[:5]:\n",
    "    print(f\"{token:>15}: {score_diff:.3f}\")\n",
    "\n",
    "# Create cloze exercise\n",
    "def create_cloze(text, num_gaps=3):\n",
    "    candidates = analyze_context_windows(text)\n",
    "    tokens_to_blank = [token for token, _ in candidates[:num_gaps]]\n",
    "    \n",
    "    # Create exercise by replacing selected tokens with gaps\n",
    "    cloze_text = text\n",
    "    for token in tokens_to_blank:\n",
    "        cloze_text = cloze_text.replace(token, \"_____\")\n",
    "    \n",
    "    return cloze_text, tokens_to_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7940cbdf-3505-4475-9db6-0413c1d29cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exercise:\n",
      "The _____ chased _____ mouse through _____ garden while _____ dog slept peacefully under _____ tree.\n",
      "\n",
      "Answers: ['the', 'cat', 'the']\n"
     ]
    }
   ],
   "source": [
    "# Create and print a cloze exercise\n",
    "text = \"The cat chased the mouse through the garden while the dog slept peacefully under the tree.\"\n",
    "exercise, answers = create_cloze(text, num_gaps=3)\n",
    "print(\"\\nExercise:\")\n",
    "print(exercise)\n",
    "print(\"\\nAnswers:\", answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cb9a864-8f4a-46d1-b283-09f49e09bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class GapCandidate:\n",
    "    word: str\n",
    "    start_idx: int\n",
    "    end_idx: int\n",
    "    local_rtd_score: float\n",
    "    global_rtd_score: float\n",
    "    score_ratio: float\n",
    "\n",
    "class ClozeGenerator:\n",
    "    def __init__(self, model_name: str = \"microsoft/deberta-v3-base\"):\n",
    "        \"\"\"\n",
    "        Initialize the Cloze generator with a DeBERTa-v3 model.\n",
    "        DeBERTa-v3 was trained with RTD (Replaced Token Detection) objective.\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        # Load model for token classification to get RTD scores\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2  # Binary classification: original vs replaced\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_rtd_scores(self, text: str, word_positions: List[tuple], context: str = None) -> List[float]:\n",
    "        \"\"\"\n",
    "        Get RTD (Replaced Token Detection) scores for words at specified positions.\n",
    "        A higher score indicates the model believes the token is original (not replaced).\n",
    "        \"\"\"\n",
    "        # Prepare text with context if provided\n",
    "        full_text = f\"{context} {text}\" if context else text\n",
    "        \n",
    "        # Tokenize the full text\n",
    "        inputs = self.tokenizer(\n",
    "            full_text,\n",
    "            return_tensors=\"pt\",\n",
    "            return_offsets_mapping=True  # Get character positions for each token\n",
    "        )\n",
    "\n",
    "        # Get offset mapping to align character positions with tokens\n",
    "        offset_map = inputs.pop(\"offset_mapping\")[0].tolist()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            # Get logits for original token prediction\n",
    "            logits = outputs.logits[0]  # Shape: [sequence_length, 2]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            original_probs = probs[:, 1]  # Probability of being original\n",
    "                \n",
    "        # Calculate scores for each word position\n",
    "        scores = []\n",
    "        for word_start, word_end in word_positions:\n",
    "            # Adjust positions if context was added\n",
    "            if context:\n",
    "                word_start += len(context) + 1\n",
    "                word_end += len(context) + 1\n",
    "            \n",
    "            # Find tokens that overlap with the word\n",
    "            token_scores = []\n",
    "            for token_idx, (token_start, token_end) in enumerate(offset_map):\n",
    "                if token_end <= word_start:\n",
    "                    continue\n",
    "                if token_start >= word_end:\n",
    "                    break\n",
    "                token_scores.append(original_probs[token_idx].item())\n",
    "            \n",
    "            # Average the scores for all tokens in the word\n",
    "            word_score = sum(token_scores) / len(token_scores) if token_scores else 0\n",
    "            scores.append(word_score)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def find_gap_candidates(self, text: str, min_word_length: int = 4) -> List[GapCandidate]:\n",
    "        \"\"\"\n",
    "        Find suitable gap candidates in the text by comparing local and global RTD scores.\n",
    "        \"\"\"\n",
    "        # Find all words and their positions in the full text\n",
    "        words_and_positions = [\n",
    "            (m.group(), m.start(), m.end())\n",
    "            for m in re.finditer(r'\\b\\w+\\b', text)\n",
    "        ]\n",
    "        \n",
    "        # Filter out short words and numbers\n",
    "        valid_words = [\n",
    "            (word, start, end) \n",
    "            for word, start, end in words_and_positions\n",
    "            if len(word) >= min_word_length and not word.isdigit()\n",
    "        ]\n",
    "        \n",
    "        if not valid_words:\n",
    "            return []\n",
    "        \n",
    "        # Split positions for batch processing\n",
    "        words, starts, ends = zip(*valid_words)\n",
    "        positions = list(zip(starts, ends))\n",
    "        \n",
    "        # Get RTD scores for each word in local and global context\n",
    "        local_scores = []\n",
    "        for start, end in positions:\n",
    "            # Get local context (sentence containing the word)\n",
    "            sentence_bounds = self._get_sentence_bounds(text, start)\n",
    "            local_text = text[sentence_bounds[0]:sentence_bounds[1]]\n",
    "            local_pos = [(start - sentence_bounds[0], end - sentence_bounds[0])]\n",
    "            score = self.get_rtd_scores(local_text, local_pos)[0]\n",
    "            local_scores.append(score)\n",
    "        \n",
    "        # Get global scores using full text context\n",
    "        global_scores = self.get_rtd_scores(text, positions)\n",
    "        \n",
    "        # Create candidates\n",
    "        candidates = []\n",
    "        for i, (word, start, end) in enumerate(valid_words):\n",
    "            local_score = local_scores[i]\n",
    "            global_score = global_scores[i]\n",
    "            \n",
    "            # Calculate ratio (lower means word is more replaceable in local context)\n",
    "            score_ratio = local_score / global_score if global_score > 0 else float('inf')\n",
    "            \n",
    "            candidates.append(GapCandidate(\n",
    "                word=word,\n",
    "                start_idx=start,\n",
    "                end_idx=end,\n",
    "                local_rtd_score=local_score,\n",
    "                global_rtd_score=global_score,\n",
    "                score_ratio=score_ratio\n",
    "            ))\n",
    "        \n",
    "        return sorted(candidates, key=lambda x: x.score_ratio)\n",
    "\n",
    "    def _get_sentence_bounds(self, text: str, position: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Find the start and end positions of the sentence containing the given position.\n",
    "        \"\"\"\n",
    "        sentence_end = text.find('.', position)\n",
    "        if sentence_end == -1:\n",
    "            sentence_end = len(text)\n",
    "        \n",
    "        sentence_start = text.rfind('.', 0, position)\n",
    "        if sentence_start == -1:\n",
    "            sentence_start = 0\n",
    "        else:\n",
    "            sentence_start += 1  # Move past the period\n",
    "            \n",
    "        return (sentence_start, sentence_end)\n",
    "\n",
    "    def create_cloze_exercise(self, text: str, num_gaps: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Create an open cloze exercise by selecting the best gap candidates.\n",
    "        \"\"\"\n",
    "        candidates = self.find_gap_candidates(text)\n",
    "        selected_candidates = candidates[:num_gaps]\n",
    "        \n",
    "        # Sort by position to process from end to beginning\n",
    "        selected_candidates.sort(key=lambda x: x.start_idx, reverse=True)\n",
    "        \n",
    "        cloze_text = text\n",
    "        for candidate in selected_candidates:\n",
    "            gap_marker = \"_____\"\n",
    "            cloze_text = (cloze_text[:candidate.start_idx] + \n",
    "                         gap_marker + \n",
    "                         cloze_text[candidate.end_idx:])\n",
    "        \n",
    "        return cloze_text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf]",
   "language": "python",
   "name": "conda-env-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
