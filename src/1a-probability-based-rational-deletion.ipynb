{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670e1086-2b3b-4e72-9a38-c21463a2d53a",
   "metadata": {},
   "source": [
    "# Rational Deletion\n",
    "\n",
    "The following approach is adapted from [Ondov et al. (2024)](https://aclanthology.org/2024.naacl-long.220/).\n",
    "\n",
    "The basic idea is to measure word probabilities with a masked language model in two ways: the first probability estimate uses the entire context of the Cloze passage and the second probability estimate uses only the local (sentence) context. We want words that are predictable given the full context, but cannot be easily guessed using only the local context. The distance between these two probability estimates indicates whether the word is more predictable in the full context than the local context.\n",
    "\n",
    "We include an alternative implementation that conditions global probability on the the entire page, rather than just the Cloze passage. iTELL Cloze exercises are source-dependent and intended to assess comprehension of the page. This means that Cloze gaps needn't be guessable from the Cloze passage itself, if they are guessable based on the larger page context.\n",
    "\n",
    "The approach handles simultaneous masking of subword tokens and allows for the following configuration:\n",
    "  - Target number of blanks to generate\n",
    "  - Minimum distance between blanks\n",
    "  - Blacklisting of part of speech tags to prevent masking of high entropy words like proper nouns and numbers\n",
    "\n",
    "We also collect predictions for other possible words. This includes a greedy search algorithm to identify whole-word predictions in the event of sub-word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cc5e3d4-0729-45d8-8f76-9bf6ef47f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pprint import pp\n",
    "\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a674959-7b8d-432b-b58b-7c0784566bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RationalClozeGenerator:\n",
    "    def __init__(self, model_name: str = \"answerdotai/ModernBERT-large\"):\n",
    "        # Load SpaCy for sentence splitting and preprocessing\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.min_blank_distance = 7  # Minimum distance between blanks\n",
    "\n",
    "        # Minimum predictability of alternatives\n",
    "        # Use log probs to avoid underflow\n",
    "        self.min_predictability = np.log(0.05)\n",
    "\n",
    "        # Part-of-Speech Blacklist (do not delete these words)\n",
    "        self.blacklist = [\n",
    "            \"PROPN\",  # Proper nouns\n",
    "            \"NUM\",  # Numbers\n",
    "            \"PUNCT\",  # Punctuation\n",
    "            \"SYM\",  # Symbols\n",
    "            \"X\",  # Other\n",
    "        ]\n",
    "\n",
    "    def _get_leading_ws_tokens(self, doc: Doc) -> list[str]:\n",
    "        \"\"\"The ModernBERT Tokenizer will work fine if we give it tokens with leading spaces.\n",
    "        SpaCy normally handles whitespace in terms of trailing space.\"\"\"\n",
    "        if not len(doc):\n",
    "            return [\"\"]\n",
    "\n",
    "        tokens = [doc[0].text]\n",
    "        # For tokens after the 0th, prepend trailing whitespace from the previous token.\n",
    "        tokens += [doc[i - 1].whitespace_ + doc[i].text for i in range(1, len(doc))]\n",
    "        return tokens\n",
    "\n",
    "    def get_token_mappings(self, tokens: list[str]) -> dict[int, list[int]]:\n",
    "        \"\"\"Get mappings between word positions and token positions\"\"\"\n",
    "        # Tokenize while keeping track of word IDs\n",
    "        tokenized = self.tokenizer(\n",
    "            tokens, return_tensors=\"pt\", is_split_into_words=True\n",
    "        )\n",
    "        word_ids = tokenized.word_ids()\n",
    "\n",
    "        # Create mapping from word position to token positions\n",
    "        word_to_tokens = defaultdict(list)\n",
    "\n",
    "        for token_idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx is not None:\n",
    "                word_to_tokens[word_idx].append(token_idx)\n",
    "\n",
    "        return word_to_tokens\n",
    "\n",
    "    def get_masked_logits(\n",
    "        self, tokens: list[str], mask_idx: int\n",
    "    ) -> tuple[torch.Tensor, int]:\n",
    "        \"\"\"Get model logits for a masked position in text\"\"\"\n",
    "        # Get the word tokens and their alignment info\n",
    "        word_to_tokens = self.get_token_mappings(tokens)\n",
    "\n",
    "        # Find all token positions for the word we want to mask\n",
    "        token_positions = word_to_tokens[mask_idx]\n",
    "\n",
    "        # Create masked version of the text\n",
    "        input_ids = self.tokenizer(\n",
    "            tokens, is_split_into_words=True, return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "        masked_ids = input_ids.clone()\n",
    "\n",
    "        # ID of the first subword token that we masked\n",
    "        first_token_id = input_ids[token_positions[0]]\n",
    "\n",
    "        # Mask all tokens corresponding to our target word\n",
    "        masked_ids[token_positions] = self.tokenizer.mask_token_id\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = self.model(input_ids.unsqueeze(0).to(self.device))\n",
    "\n",
    "        # Get logits\n",
    "        logits = outputs.logits[0, token_positions, :]\n",
    "\n",
    "        return logits, first_token_id\n",
    "\n",
    "    def get_contextuality_score(\n",
    "        self, page_doc: Doc, summary_doc: Doc, sent: Span, tok: Token, method: str = \"kl\"\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate contextuality score for a word position using full page context\n",
    "\n",
    "        Args:\n",
    "            page_doc: The full page text as a spaCy Doc\n",
    "            summary_doc: The summary text as a spaCy Doc\n",
    "            sent: The sentence from the summary containing the token\n",
    "            tok: The token from the summary to evaluate\n",
    "            method: \"kl\" for kl-divergence or \"contextuality\" for contextuality score\n",
    "\n",
    "        Returns:\n",
    "            Contextuality score\n",
    "        \"\"\"\n",
    "\n",
    "        # Get logits for both full text and sentence text\n",
    "        # For the full text context, we use the page + summary\n",
    "        full_toks = self._get_leading_ws_tokens(page_doc) + self._get_leading_ws_tokens(\n",
    "            summary_doc\n",
    "        )\n",
    "        full_pos = len(page_doc) + tok.i  # Position of token in full document\n",
    "        full_logits, word_id = self.get_masked_logits(full_toks, full_pos)\n",
    "\n",
    "        # For the local context, we use just the sentence from the summary\n",
    "        sent_pos = tok.i - sent.start  # Position of token in the sentence\n",
    "        sent_logits, _ = self.get_masked_logits([tok.text for tok in sent], sent_pos)\n",
    "\n",
    "        # Calculate probabilities using first sub-word token\n",
    "        full_probs = torch.softmax(full_logits[0], dim=0)\n",
    "        sent_probs = torch.softmax(sent_logits[0], dim=0)\n",
    "\n",
    "        p = full_probs[word_id]\n",
    "        q = sent_probs[word_id]\n",
    "\n",
    "        if method == \"kl\":\n",
    "            # KL-divergence is p*log(p/q)\n",
    "            score = float(p*torch.log2(p/q))\n",
    "        elif method == \"contextuality\":\n",
    "            # Contextuality is distance between full-text and sentence probability\n",
    "            score = float(p - q)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown method.\")\n",
    "\n",
    "        return score\n",
    "\n",
    "    def choose_blank_positions(\n",
    "        self, page_doc: Doc, summary_doc: Doc, num_blanks: int\n",
    "    ) -> list[int]:\n",
    "        \"\"\"Choose positions to blank in the summary based on contextuality scores with full page\"\"\"\n",
    "        scores = []\n",
    "        valid_positions = []\n",
    "\n",
    "        # Calculate scores for each position in the summary\n",
    "        for i, sent in enumerate(summary_doc.sents):\n",
    "            if i == 0:\n",
    "                continue  # Skip first sentence\n",
    "            for tok in sent:\n",
    "                if (\n",
    "                    len(tok.text) < 3\n",
    "                    or tok.pos_ in self.blacklist\n",
    "                    or tok.is_stop\n",
    "                    or not tok.text.isalpha()\n",
    "                ):\n",
    "                    scores.append(-float(\"inf\"))\n",
    "                else:\n",
    "                    # Calculate contextuality using both the full page and summary\n",
    "                    score = self.get_contextuality_score(\n",
    "                        page_doc, summary_doc, sent, tok\n",
    "                    )\n",
    "                    scores.append(score)\n",
    "                valid_positions.append(tok.i)\n",
    "\n",
    "        # Convert to numpy for easier manipulation\n",
    "        scores = np.array(scores)\n",
    "\n",
    "        # Choose positions greedily while maintaining minimum distance\n",
    "        positions = []\n",
    "        for _ in range(num_blanks):\n",
    "            if np.all(scores == -float(\"inf\")):\n",
    "                break\n",
    "\n",
    "            # Choose highest scoring position\n",
    "            idx = np.argmax(scores)\n",
    "            pos = valid_positions[idx]\n",
    "            positions.append(pos)\n",
    "\n",
    "            # Zero out scores within minimum distance\n",
    "            start = max(0, idx - self.min_blank_distance)\n",
    "            end = min(len(scores), idx + self.min_blank_distance + 1)\n",
    "            scores[start:end] = -float(\"inf\")\n",
    "\n",
    "        return sorted(positions)\n",
    "\n",
    "    def get_alternates(self, tokens: list[str], topk=5) -> list[dict]:\n",
    "        \"\"\"Get top k predictions for the masked positions in tokens\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries, one per masked position, with candidate words and their probabilities\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        # Find all mask positions\n",
    "        mask_positions = [i for i, token in enumerate(tokens) if token == \"[MASK]\"]\n",
    "\n",
    "        for mask_pos in mask_positions:\n",
    "            word_candidates = {}\n",
    "\n",
    "            # Try different mask lengths (1, 2, or 3 tokens)\n",
    "            for mask_length in range(1, 4):\n",
    "                # Replace the single mask with multiple if needed\n",
    "                masked_tokens = (\n",
    "                    tokens[:mask_pos]\n",
    "                    + [\"[MASK]\"] * mask_length\n",
    "                    + tokens[mask_pos + 1 :]\n",
    "                )\n",
    "\n",
    "                # Get initial predictions for first token\n",
    "                current_candidates = []\n",
    "                logits, _ = self.get_masked_logits(masked_tokens, mask_pos)\n",
    "                probs = torch.softmax(logits[0], dim=0)\n",
    "                top_values, top_indices = torch.topk(probs, topk)\n",
    "\n",
    "                # Start with first token candidates\n",
    "                for idx, prob in zip(top_indices.tolist(), top_values.tolist()):\n",
    "                    current_candidates.append(([idx], prob))\n",
    "\n",
    "                # Build up multi-token predictions if needed\n",
    "                for token_idx in range(1, mask_length):\n",
    "                    new_candidates = []\n",
    "                    for token_ids, prob in current_candidates:\n",
    "                        # Fill in what we've predicted so far\n",
    "                        partial_filled = tokens.copy()\n",
    "                        filled_text = self.tokenizer.decode(token_ids)\n",
    "                        remaining_masks = mask_length - token_idx\n",
    "\n",
    "                        partial_filled = (\n",
    "                            tokens[:mask_pos]\n",
    "                            + [filled_text]\n",
    "                            + [\"[MASK]\"] * remaining_masks\n",
    "                            + tokens[mask_pos + 1 :]\n",
    "                        )\n",
    "\n",
    "                        # Get prediction for next position\n",
    "                        next_logits, _ = self.get_masked_logits(\n",
    "                            partial_filled, mask_pos + 1\n",
    "                        )\n",
    "                        next_probs = torch.softmax(next_logits[0], dim=0)\n",
    "                        next_values, next_indices = torch.topk(next_probs, 1)\n",
    "\n",
    "                        # Add to candidates\n",
    "                        new_token_ids = token_ids + [next_indices[0].item()]\n",
    "                        new_prob = prob * next_values[0].item()\n",
    "                        new_candidates.append((new_token_ids, new_prob))\n",
    "\n",
    "                    current_candidates = new_candidates\n",
    "\n",
    "                # Add final decoded words\n",
    "                for token_ids, prob in current_candidates:\n",
    "                    word = self.tokenizer.decode(token_ids).strip()\n",
    "                    if \" \" in word:\n",
    "                        # Word contains a space (is actually multiple words)\n",
    "                        continue\n",
    "                    if word not in word_candidates or prob > word_candidates[word]:\n",
    "                        word_candidates[word] = prob\n",
    "\n",
    "            # Sort candidates by probability\n",
    "            sorted_candidates = sorted(\n",
    "                word_candidates.items(), key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "            predictions.append({word: prob for word, prob in sorted_candidates[:topk]})\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def generate_cloze(\n",
    "        self, page_text: str, summary_text: str, num_blanks: int\n",
    "    ) -> tuple[str, list[str], list[dict[str, float]]]:\n",
    "        \"\"\"Generate a cloze text from summary using page for context\n",
    "\n",
    "        Args:\n",
    "            page_text: The full page text\n",
    "            summary_text: The summary text to create gaps in\n",
    "            num_blanks: Number of blanks to create\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (cloze_text, answers, alternates)\n",
    "        \"\"\"\n",
    "        # Process both texts\n",
    "        page_doc = self.nlp(page_text)\n",
    "        summary_doc = self.nlp(summary_text)\n",
    "\n",
    "        # Choose positions to blank in the summary\n",
    "        masked_positions = self.choose_blank_positions(\n",
    "            page_doc, summary_doc, num_blanks\n",
    "        )\n",
    "\n",
    "        # Get the answers (the original words that will be blanked)\n",
    "        answers = [summary_doc[pos].text for pos in masked_positions]\n",
    "\n",
    "        # Replace tokens with mask\n",
    "        summary_tokens = np.array(self._get_leading_ws_tokens(summary_doc))\n",
    "        summary_tokens[masked_positions] = \"[MASK]\"\n",
    "        summary_tokens = summary_tokens.tolist()\n",
    "\n",
    "        # Construct cloze token input for gap predictions\n",
    "        cloze_tokens = self._get_leading_ws_tokens(page_doc) + summary_tokens\n",
    "\n",
    "        # Get gap predictions based on the full page context\n",
    "        alternates = self.get_alternates(cloze_tokens)\n",
    "\n",
    "        # Replace words with blanks in the summary\n",
    "        cloze_text = \"\"\n",
    "        for tok in summary_doc:\n",
    "            if tok.i in masked_positions:\n",
    "                cloze_text += \"_\" * len(tok.text) + tok.whitespace_\n",
    "            else:\n",
    "                cloze_text += tok.text_with_ws\n",
    "\n",
    "        return cloze_text, answers, alternates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d70440-6fc3-4957-8620-087b616a59f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = RationalClozeGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb920209-f3a2-4f37-bc59-02df9b16c396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloze text:\n",
      "The Cloze procedure, first introduced by Taylor, is a widely used method for creating reading \n",
      "comprehension tests inspired by the Gestalt principle of closure. Though many variations have been \n",
      "introduced and _______, the core concept is to mask words in _____ and task the subject with providing \n",
      "the missing _____.\n",
      "\n",
      "Answers:\n",
      "{'studied': {'used': 0.21,\n",
      "             'tested': 0.13,\n",
      "             'developed': 0.11,\n",
      "             'refined': 0.09,\n",
      "             'adapted': 0.06},\n",
      " 'prose': {'text': 0.59,\n",
      "           'context': 0.09,\n",
      "           'sentences': 0.07,\n",
      "           'texts': 0.03,\n",
      "           'isolation': 0.02},\n",
      " 'words': {'words': 0.5,\n",
      "           'information': 0.17,\n",
      "           'word': 0.17,\n",
      "           'text': 0.02,\n",
      "           'parts': 0.01}}\n"
     ]
    }
   ],
   "source": [
    "page_text = \"\"\"A cloze test (also cloze deletion test or occlusion test) is an exercise, test, or assessment in which a portion of text is masked and the participant is asked to fill in the masked portion of text. Cloze tests require the ability to understand the context and vocabulary in order to identify the correct language or part of speech that belongs in the deleted passages. This exercise is commonly administered for the assessment of native and second language learning and instruction.\"\"\"\n",
    "# page_text = \"\"\"Alphabet Soup is the best kind of soup.\"\"\"\n",
    "\n",
    "summary_text = \"\"\"The Cloze procedure, first introduced by Taylor, is a widely used method for creating reading \n",
    "comprehension tests inspired by the Gestalt principle of closure. Though many variations have been \n",
    "introduced and studied, the core concept is to mask words in prose and task the subject with providing \n",
    "the missing words.\"\"\"\n",
    "\n",
    "cloze_text, answers, alternates = generator.generate_cloze(\n",
    "    page_text, summary_text, num_blanks=6\n",
    ")\n",
    "print(\"Cloze text:\")\n",
    "print(cloze_text)\n",
    "print(\"\\nAnswers:\")\n",
    "answer_dict = {\n",
    "    answer: {word: round(prob, 2) for word, prob in pred.items()}\n",
    "    for answer, pred in zip(answers, alternates)\n",
    "}\n",
    "pp(answer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0509b5c-8b69-4e61-8baa-b0b8e4adc912",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_summaries = {}\n",
    "df = pd.read_csv(\"../data/itell-pages.csv\")\n",
    "for page in df.itertuples():\n",
    "    if page.summary:\n",
    "        # print(page[\"PageSummary\"])\n",
    "        # page_summaries[page[\"Slug\"]] = page[\"PageSummary\"]\n",
    "        print(page.page)\n",
    "        print(\"=\" * 80)\n",
    "        cloze_text, answers, preds = generator.generate_cloze(\n",
    "            page.summary, page.text, num_blanks=6\n",
    "        )\n",
    "        print(\"Cloze text:\")\n",
    "        print(cloze_text)\n",
    "        print(\"\\nAnswers:\")\n",
    "        answer_dict = {\n",
    "            answer: {word: round(prob, 2) for word, prob in pred.items()}\n",
    "            for answer, pred in zip(answers, preds)\n",
    "        }\n",
    "        pp(answer_dict)\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a5827-4f91-4fef-bf8f-e162ed857c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf]",
   "language": "python",
   "name": "conda-env-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
