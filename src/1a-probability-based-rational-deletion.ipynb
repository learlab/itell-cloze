{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670e1086-2b3b-4e72-9a38-c21463a2d53a",
   "metadata": {},
   "source": [
    "# Rational Deletion\n",
    "\n",
    "The following approach is adapted from [Ondov et al. (2024)](https://aclanthology.org/2024.naacl-long.220/).\n",
    "\n",
    "The basic idea is to measure word probabilities with a masked language model in two ways: the first probability estimate uses the entire context of the Cloze passage and the second probability estimate uses only the local (sentence) context. We want words that are predictable given the full context, but cannot be easily guessed using only the local context. The distance between these two probability estimates indicates whether the word is more predictable in the full context than the local context.\n",
    "\n",
    "We include an alternative implementation that conditions global probability on the the entire page, rather than just the Cloze passage. iTELL Cloze exercises are source-dependent and intended to assess comprehension of the page. This means that Cloze gaps needn't be guessable from the Cloze passage itself, if they are guessable based on the larger page context.\n",
    "\n",
    "The approach handles simultaneous masking of subword tokens and allows for the following configuration:\n",
    "  - Target number of blanks to generate\n",
    "  - Minimum distance between blanks\n",
    "  - Blacklisting of part of speech tags to prevent masking of high entropy words like proper nouns and numbers\n",
    "\n",
    "We also collect predictions for other possible words. This includes a greedy search algorithm to identify whole-word predictions in the event of sub-word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cc5e3d4-0729-45d8-8f76-9bf6ef47f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "from pprint import pp\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d55617b-6a56-4e0d-b2ca-9e3d1b68646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73316d25-3881-478d-8ad9-0e994077586f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [50281, 1552, 310, 247, 1071, 3425, 15, 50282], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    tokenizer(\"This is a test sequence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28cf8468-1c4a-4982-9ece-f75744372a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c89e4790-7e9f-4550-943c-c6f5e5db499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I. Synechdoche this is a test sequence with multitoken lexical units that I ask about.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f3277f8f-ab3a-4087-b6fd-009ba6ea8aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c555bd43-3e15-4250-8360-c3251c4a00d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50281, 42, 15, 5619, 570, 348, 7152, 248, 436, 310, 247, 1071, 3425, 342, 1554, 262, 5097, 26752, 474, 5085, 326, 309, 1642, 670, 15, 50282]\n",
      "['[CLS]', 'I', '.', 'ĠSy', 'ne', 'ch', 'doc', 'he', 'Ġthis', 'Ġis', 'Ġa', 'Ġtest', 'Ġsequence', 'Ġwith', 'Ġmult', 'it', 'oken', 'Ġlex', 'ical', 'Ġunits', 'Ġthat', 'ĠI', 'Ġask', 'Ġabout', '.', '[SEP]']\n",
      "[CLS]I. Synechdoche this is a test sequence with multitoken lexical units that I ask about.[SEP]\n"
     ]
    }
   ],
   "source": [
    "mb_tokens = tokenizer(text).input_ids\n",
    "print(mb_tokens)\n",
    "print(tokenizer.convert_ids_to_tokens(mb_tokens))\n",
    "print(tokenizer.decode(mb_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6061e73-10d0-4d7c-8dd8-e8306545dfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I.', ' Synechdoche', ' this', '[MASK]', ' a', ' test', ' sequence', '[MASK]', ' multitoken', '[MASK]', ' units', ' that', ' I', ' ask', ' about', '.']\n",
      "[50281, 42, 15, 5619, 570, 348, 7152, 248, 436, 50284, 247, 1071, 3425, 50284, 1554, 262, 5097, 50284, 5085, 326, 309, 1642, 670, 15, 50282]\n",
      "['[CLS]', 'I', '.', 'ĠSy', 'ne', 'ch', 'doc', 'he', 'Ġthis', '[MASK]', 'Ġa', 'Ġtest', 'Ġsequence', '[MASK]', 'Ġmult', 'it', 'oken', '[MASK]', 'Ġunits', 'Ġthat', 'ĠI', 'Ġask', 'Ġabout', '.', '[SEP]']\n",
      "[CLS]I. Synechdoche this[MASK] a test sequence[MASK] multitoken[MASK] units that I ask about.[SEP]\n"
     ]
    }
   ],
   "source": [
    "spacy_tokens = [doc[0].text] + [doc[i-1].whitespace_ + doc[i].text for i in range(1, len(doc))]\n",
    "spacy_tokens = np.array(spacy_tokens)\n",
    "spacy_tokens[[3, 7, 9]] = \"[MASK]\"\n",
    "spacy_tokens = spacy_tokens.tolist()\n",
    "print(spacy_tokens)\n",
    "mb_tokens = tokenizer(spacy_tokens, is_split_into_words=True).input_ids\n",
    "print(mb_tokens)\n",
    "print(tokenizer.convert_ids_to_tokens(mb_tokens))\n",
    "print(tokenizer.decode(mb_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a674959-7b8d-432b-b58b-7c0784566bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RationalClozeGenerator:\n",
    "    def __init__(self, model_name: str = \"answerdotai/ModernBERT-large\"):\n",
    "        # Load SpaCy for sentence splitting and preprocessing\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.min_blank_distance = 7 # Minimum distance between blanks\n",
    "        \n",
    "        # Minimum predictability of alternatives\n",
    "        # Use log probs to avoid underflow\n",
    "        self.min_predictability = np.log(0.05)\n",
    "\n",
    "        # Part-of-Speech Blacklist (do not delete these words)\n",
    "        self.blacklist = [\n",
    "            \"PROPN\", # Proper nouns\n",
    "            \"NUM\", # Numbers\n",
    "            \"PUNCT\", # Punctuation\n",
    "            \"SYM\", # Symbols\n",
    "            \"X\", # Other\n",
    "        ]\n",
    "\n",
    "\n",
    "    def _get_leading_ws_tokens(self, doc: Doc) -> list[str]:\n",
    "        \"\"\"The ModernBERT Tokenizer will work fine if we give it tokens with leading spaces.\n",
    "        SpaCy normally handles whitespace in terms of trailing space.\"\"\"\n",
    "        tokens = [doc[0].text]\n",
    "        # For tokens after the 0th, prepend trailing whitespace from the previous token.\n",
    "        tokens += [doc[i-1].whitespace_ + doc[i].text for i in range(1, len(doc))]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def _is_complete_word(self, token_ids):\n",
    "        \"\"\"Check if the token sequence forms a complete word\"\"\"\n",
    "        decoded = self.tokenizer.decode(token_ids)\n",
    "        # A complete word shouldn't have spaces within it\n",
    "        # But might have spaces after it due to tokenizer behavior\n",
    "        return \" \" not in decoded.strip() and decoded.strip() != \"\"\n",
    "\n",
    "\n",
    "    def get_token_mappings(self, tokens: list[str]) -> dict[int, list[int]]:\n",
    "        \"\"\"Get mappings between word positions and token positions\"\"\"\n",
    "        # Tokenize while keeping track of word IDs\n",
    "        tokenized = self.tokenizer(tokens, return_tensors=\"pt\", is_split_into_words=True)\n",
    "        word_ids = tokenized.word_ids()\n",
    "        \n",
    "        # Create mapping from word position to token positions\n",
    "        word_to_tokens = defaultdict(list)\n",
    "        \n",
    "        for token_idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx is not None:\n",
    "                word_to_tokens[word_idx].append(token_idx)\n",
    "\n",
    "        return word_to_tokens\n",
    "\n",
    "    def get_masked_logits(self, tokens: list[str], mask_idx: int) -> tuple[torch.Tensor, int]:\n",
    "        \"\"\"Get model logits for a masked position in text\"\"\"\n",
    "        # Get the word tokens and their alignment info\n",
    "        word_to_tokens = self.get_token_mappings(tokens)\n",
    "        \n",
    "        # Find all token positions for the word we want to mask\n",
    "        token_positions = word_to_tokens[mask_idx]\n",
    "        \n",
    "        # Create masked version of the text\n",
    "        input_ids = self.tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\").input_ids[0]\n",
    "        masked_ids = input_ids.clone()\n",
    "\n",
    "        # ID of the first subword token that we masked\n",
    "        first_token_id = input_ids[token_positions[0]]\n",
    "\n",
    "        # Mask all tokens corresponding to our target word\n",
    "        masked_ids[token_positions] = self.tokenizer.mask_token_id\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = self.model(input_ids.unsqueeze(0).to(self.device))\n",
    "            \n",
    "        # Get logits \n",
    "        logits = outputs.logits[0, token_positions, :]\n",
    "\n",
    "        return logits, first_token_id\n",
    "\n",
    "    def get_contextuality_score(self, page_doc: Doc, summary_doc: Doc, sent: Span, tok: Token) -> float:\n",
    "        \"\"\"Calculate contextuality score for a word position using full page context\n",
    "        \n",
    "        Args:\n",
    "            page_doc: The full page text as a spaCy Doc\n",
    "            summary_doc: The summary text as a spaCy Doc\n",
    "            sent: The sentence from the summary containing the token\n",
    "            tok: The token from the summary to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            Contextuality score\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get logits for both full text and sentence text\n",
    "        # For the full text context, we use the page + summary\n",
    "        full_toks = self._get_leading_ws_tokens(page_doc) + self._get_leading_ws_tokens(summary_doc)\n",
    "        full_pos = len(page_doc) + tok.i  # Position of token in full document\n",
    "        full_logits, word_id = self.get_masked_logits(full_toks, full_pos)\n",
    "        \n",
    "        # For the local context, we use just the sentence from the summary\n",
    "        sent_pos = tok.i - sent.start # Position of token in the sentence\n",
    "        sent_logits, _ = self.get_masked_logits([tok.text for tok in sent], sent_pos)\n",
    "        \n",
    "        # Calculate probabilities using first sub-word token\n",
    "        full_probs = torch.softmax(full_logits[0], dim=0)\n",
    "        sent_probs = torch.softmax(sent_logits[0], dim=0)\n",
    "\n",
    "        # Contextuality is distance between full-text and sentence probability\n",
    "        if float(full_probs[word_id].log()) > self.min_predictability:\n",
    "            score = float(full_probs[word_id].log() - sent_probs[word_id].log())\n",
    "        else:\n",
    "            score = float(\"-inf\")\n",
    "\n",
    "        return score\n",
    "\n",
    "    def choose_blank_positions(self, page_doc: Doc, summary_doc: Doc, num_blanks: int) -> list[int]:\n",
    "        \"\"\"Choose positions to blank in the summary based on contextuality scores with full page\"\"\"\n",
    "        scores = []\n",
    "        valid_positions = []\n",
    "        \n",
    "        # Calculate scores for each position in the summary\n",
    "        for i, sent in enumerate(summary_doc.sents):\n",
    "            if i == 0:\n",
    "                continue  # Skip first sentence\n",
    "            for tok in sent:\n",
    "                if (\n",
    "                    len(tok.text) < 3\n",
    "                    or tok.pos_ in self.blacklist\n",
    "                    or tok.is_stop\n",
    "                    or not tok.text.isalpha()\n",
    "                   ):\n",
    "                    scores.append(-float('inf'))\n",
    "                else:\n",
    "                    # Calculate contextuality using both the full page and summary\n",
    "                    score = self.get_contextuality_score(page_doc, summary_doc, sent, tok)\n",
    "                    scores.append(score)\n",
    "                valid_positions.append(tok.i)\n",
    "                \n",
    "        # Convert to numpy for easier manipulation\n",
    "        scores = np.array(scores)\n",
    "        \n",
    "        # Choose positions greedily while maintaining minimum distance\n",
    "        positions = []\n",
    "        for _ in range(num_blanks):\n",
    "            if np.all(scores == -float('inf')):\n",
    "                break\n",
    "                \n",
    "            # Choose highest scoring position\n",
    "            idx = np.argmax(scores)\n",
    "            pos = valid_positions[idx]\n",
    "            positions.append(pos)\n",
    "            \n",
    "            # Zero out scores within minimum distance\n",
    "            start = max(0, idx - self.min_blank_distance)\n",
    "            end = min(len(scores), idx + self.min_blank_distance + 1)\n",
    "            scores[start:end] = -float('inf')\n",
    "\n",
    "        return sorted(positions)\n",
    "\n",
    "\n",
    "    def get_alternates(self, tokens: list[str], topk=5) -> list[dict]:\n",
    "        \"\"\"Get top k predictions for the masked positions in tokens\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries, one per masked position, with candidate words and their probabilities\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        # Find all mask positions\n",
    "        mask_positions = [i for i, token in enumerate(tokens) if token == \"[MASK]\"]\n",
    "        \n",
    "        for mask_pos in mask_positions:\n",
    "            word_candidates = {}\n",
    "            \n",
    "            # Try different mask lengths (1, 2, or 3 tokens)\n",
    "            for mask_length in range(1, 4):\n",
    "                # Replace the single mask with multiple if needed\n",
    "                masked_tokens = tokens[:mask_pos] + [\"[MASK]\"] * mask_length + tokens[mask_pos+1:]\n",
    "                \n",
    "                # Get initial predictions for first token\n",
    "                current_candidates = []\n",
    "                logits, _ = self.get_masked_logits(masked_tokens, mask_pos)\n",
    "                probs = torch.softmax(logits[0], dim=0)\n",
    "                top_values, top_indices = torch.topk(probs, topk)\n",
    "                \n",
    "                # Start with first token candidates\n",
    "                for idx, prob in zip(top_indices.tolist(), top_values.tolist()):\n",
    "                    current_candidates.append(([idx], prob))\n",
    "                \n",
    "                # Build up multi-token predictions if needed\n",
    "                for token_idx in range(1, mask_length):\n",
    "                    new_candidates = []\n",
    "                    for token_ids, prob in current_candidates:\n",
    "                        # Fill in what we've predicted so far\n",
    "                        partial_filled = tokens.copy()\n",
    "                        filled_text = self.tokenizer.decode(token_ids)\n",
    "                        remaining_masks = mask_length - token_idx\n",
    "                        \n",
    "                        partial_filled = (\n",
    "                            tokens[:mask_pos] + \n",
    "                            [filled_text] + \n",
    "                            [\"[MASK]\"] * remaining_masks + \n",
    "                            tokens[mask_pos+1:]\n",
    "                        )\n",
    "                        \n",
    "                        # Get prediction for next position\n",
    "                        next_logits, _ = self.get_masked_logits(partial_filled, mask_pos + 1)\n",
    "                        next_probs = torch.softmax(next_logits[0], dim=0)\n",
    "                        next_values, next_indices = torch.topk(next_probs, 1)\n",
    "                        \n",
    "                        # Add to candidates\n",
    "                        new_token_ids = token_ids + [next_indices[0].item()]\n",
    "                        new_prob = prob * next_values[0].item()\n",
    "                        new_candidates.append((new_token_ids, new_prob))\n",
    "                    \n",
    "                    current_candidates = new_candidates\n",
    "                \n",
    "                # Add final decoded words\n",
    "                for token_ids, prob in current_candidates:\n",
    "                    word = self.tokenizer.decode(token_ids).strip()\n",
    "                    if \" \" in word:\n",
    "                        # Word contains a space (is actually multiple words)\n",
    "                        continue\n",
    "                    if word not in word_candidates or prob > word_candidates[word]:\n",
    "                        word_candidates[word] = prob\n",
    "            \n",
    "            # Sort candidates by probability\n",
    "            sorted_candidates = sorted(word_candidates.items(), key=lambda x: x[1], reverse=True)\n",
    "            predictions.append({word: prob for word, prob in sorted_candidates[:topk]})\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def generate_cloze(self, page_text: str, summary_text: str, num_blanks: int) -> tuple[str, list[str], list[dict[str, float]]]:\n",
    "        \"\"\"Generate a cloze text from summary using page for context\n",
    "        \n",
    "        Args:\n",
    "            page_text: The full page text\n",
    "            summary_text: The summary text to create gaps in\n",
    "            num_blanks: Number of blanks to create\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (cloze_text, answers, alternates)\n",
    "        \"\"\"\n",
    "        # Process both texts\n",
    "        page_doc = self.nlp(page_text)\n",
    "        summary_doc = self.nlp(summary_text)\n",
    "        \n",
    "        # Choose positions to blank in the summary\n",
    "        masked_positions = self.choose_blank_positions(page_doc, summary_doc, num_blanks)\n",
    "        \n",
    "        # Get the answers (the original words that will be blanked)\n",
    "        answers = [summary_doc[pos].text for pos in masked_positions]\n",
    "\n",
    "        # Replace tokens with mask\n",
    "        summary_tokens = np.array(self._get_leading_ws_tokens(summary_doc))\n",
    "        summary_tokens[masked_positions] = \"[MASK]\"\n",
    "        summary_tokens = summary_tokens.tolist()\n",
    "\n",
    "        # Construct cloze token input for gap predictions\n",
    "        cloze_tokens = self._get_leading_ws_tokens(page_doc) + summary_tokens\n",
    "\n",
    "        # Get gap predictions based on the full page context        \n",
    "        alternates = self.get_alternates(cloze_tokens)\n",
    "        \n",
    "        # Replace words with blanks in the summary\n",
    "        cloze_text = \"\"\n",
    "        for tok in summary_doc:\n",
    "            if tok.i in masked_positions:\n",
    "                cloze_text += \"_\" * len(tok.text) + tok.whitespace_\n",
    "            else:\n",
    "                cloze_text += tok.text_with_ws\n",
    "            \n",
    "        return cloze_text, answers, alternates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "90d70440-6fc3-4957-8620-087b616a59f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = RationalClozeGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eb920209-f3a2-4f37-bc59-02df9b16c396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloze text:\n",
      "The Cloze procedure, first introduced by Taylor, is a widely used method for creating reading \n",
      "comprehension tests inspired by the Gestalt principle of closure. Though many variations have been \n",
      "introduced and _______, the core concept is to mask words in _____ and task the subject with providing \n",
      "the missing _____.\n",
      "\n",
      "Answers:\n",
      "{'studied': {'used': 0.21,\n",
      "             'tested': 0.13,\n",
      "             'developed': 0.11,\n",
      "             'refined': 0.09,\n",
      "             'adapted': 0.06},\n",
      " 'prose': {'text': 0.59,\n",
      "           'context': 0.09,\n",
      "           'sentences': 0.07,\n",
      "           'texts': 0.03,\n",
      "           'isolation': 0.02},\n",
      " 'words': {'words': 0.5,\n",
      "           'information': 0.17,\n",
      "           'word': 0.17,\n",
      "           'text': 0.02,\n",
      "           'parts': 0.01}}\n"
     ]
    }
   ],
   "source": [
    "page_text = \"\"\"A cloze test (also cloze deletion test or occlusion test) is an exercise, test, or assessment in which a portion of text is masked and the participant is asked to fill in the masked portion of text. Cloze tests require the ability to understand the context and vocabulary in order to identify the correct language or part of speech that belongs in the deleted passages. This exercise is commonly administered for the assessment of native and second language learning and instruction.\"\"\"\n",
    "# page_text = \"\"\"Alphabet Soup is the best kind of soup.\"\"\"\n",
    "\n",
    "summary_text = \"\"\"The Cloze procedure, first introduced by Taylor, is a widely used method for creating reading \n",
    "comprehension tests inspired by the Gestalt principle of closure. Though many variations have been \n",
    "introduced and studied, the core concept is to mask words in prose and task the subject with providing \n",
    "the missing words.\"\"\"\n",
    "\n",
    "cloze_text, answers, alternates = generator.generate_cloze(page_text, summary_text, num_blanks=6)\n",
    "print(\"Cloze text:\")\n",
    "print(cloze_text)\n",
    "print(\"\\nAnswers:\")\n",
    "answer_dict = {answer: {word: round(prob,2) for word, prob in pred.items()} for answer, pred in zip(answers, alternates)}\n",
    "pp(answer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1be62889-3797-420c-a423-8d51baf3ce76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cloze procedure, first introduced by Taylor, is a widely used method for creating reading \n",
      "comprehension tests inspired by the Gestalt principle of closure. Though many __________ have been \n",
      "introduced and studied, the core _______ is to mask words in prose and ____ the subject with providing \n",
      "the missing _____.\n"
     ]
    }
   ],
   "source": [
    "page_text = \"\"\n",
    "cloze_text, answers = generator.generate_cloze(page_text, summary_text, num_blanks=6)\n",
    "print(cloze_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e0509b5c-8b69-4e61-8baa-b0b8e4adc912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-experimental-and-clinical-psychologists\n",
      "================================================================================\n",
      "Cloze text:\n",
      "Experimental psychologists, _________ holding doctoral and master's degrees, conduct scientific research in various psychology subfields, often collaborating with students at universities. While some are trained clinicians, most focus on non-clinical areas such as cognitive or ______ psychology. Their research is crucial for understanding human behavior and developing _________ knowledge, which is vital for clinical practice. The interplay between research and practice is significant, as psychological disorders are ___________ testable. The effectiveness of treatments, like psychotherapy, relies on __________ validation. The clinical psychology community debates the emphasis on empirically _________ treatments, but there is consensus on the need for a scientific approach to ensure effective diagnosis and treatment.\n",
      "\n",
      "Answers:\n",
      "{'primarily': {'primarily': 1.0},\n",
      " 'social': {'social': 0.95},\n",
      " 'empirical': {'empirical': 1.0},\n",
      " 'empirically': {'empirically': 1.0},\n",
      " 'scientific': {'scientific': 1.0},\n",
      " 'supported': {'supported': 0.88}}\n",
      "================================================================================\n",
      "12-analyzing-the-data\n",
      "================================================================================\n",
      "Cloze text:\n",
      "Descriptive statistics are tools used to organize and summarize data, including measures of _______ tendency like mean, median, and mode, as well as measures of dispersion such as range, standard deviation, and variance. These statistics help describe the central point and variability within a data set. In experimental research, means and ________ deviations are calculated to compare groups, while non-experimental research may use percentages and correlation coefficients to explore relationships between variables. Inferential statistics, on the other ____, allow researchers to draw conclusions about a population based on sample data, determining statistical significance to infer real effects. ___________ significance indicates that results are unlikely due to chance, with a typical threshold of 5% for determining significance. ___________ must be cautious of errors; a Type I error occurs when a false positive is concluded, whereas a Type II error involves missing a real effect. _________ these errors is crucial for accurate statistical conclusions.\n",
      "\n",
      "Answers:\n",
      "{'central': {'central': 0.91},\n",
      " 'standard': {'standard': 1.0},\n",
      " 'hand': {'hand': 0.92},\n",
      " 'Statistical': {'statistical': 0.53, '': 0.14},\n",
      " 'Researchers': {'residences': 1.0},\n",
      " 'Balancing': {'balancing': 0.87}}\n",
      "================================================================================\n",
      "18-6-styles-of-management\n",
      "================================================================================\n",
      "Cloze text:\n",
      "Douglas McGregor's influential book, The Human Side of Enterprise, __________ Theory X and Theory Y, which are based on Maslow's hierarchy of _____. Theory X assumes that workers are inherently lazy, motivated solely by basic needs, and require strict supervision, leading managers to adopt an authoritarian style. Conversely, Theory Y suggests that workers are self-motivated and thrive on responsibility, promoting a more _____________ management approach. William Ouchi's Theory Z integrates elements of both theories, emphasizing worker participation, skill ___________, and loyalty, drawing from American and Japanese __________ practices. These theories offer foundational insights into diverse __________ styles and intercultural organizational understanding.\n",
      "\n",
      "Answers:\n",
      "{'introduces': {'introduces': 1.0},\n",
      " 'needs': {'needs': 0.58},\n",
      " 'participative': {'participative': 0.82},\n",
      " 'development': {'development': 0.96},\n",
      " 'management': {'management': 0.95}}\n",
      "================================================================================\n",
      "18-2-how-to-understand-intercultural-communication\n",
      "================================================================================\n",
      "Cloze text:\n",
      "Edward T. Hall, an influential American anthropologist, transformed the study of intercultural communication by emphasizing individual interactions over generalized cultural ____________. His insights emerged from his multicultural experiences in the American Southwest and global travels, including his work with the U.S. State Department. Hall's contributions include focusing on local perspectives, acknowledging the importance of ________ experience, and understanding that individuals within a culture may diverge from _____________ norms. He highlighted the pitfalls of stereotypes, as explored by psychologist Gordon Allport, noting how they oversimplify ________ and hinder genuine understanding. Hall advocated for firsthand cultural experiences to ______ learning and mitigate prejudice, urging openness to diverse ____________ and personal growth through direct interaction with unfamiliar cultures.\n",
      "\n",
      "Answers:\n",
      "{'observations': {'observations': 1.0},\n",
      " 'personal': {'personal': 0.99},\n",
      " 'stereotypical': {'stereotypical': 1.0},\n",
      " 'cultures': {'cultures': 1.0},\n",
      " 'foster': {'foster': 1.0},\n",
      " 'perspectives': {'perspectives': 1.0}}\n",
      "================================================================================\n",
      "4-control-room-operator-cro-training\n",
      "================================================================================\n",
      "Cloze text:\n",
      "To qualify as a Control Room Operator (CRO), candidates must meet specific training requirements ________ in the provided document. Those who fail the CRO fundamentals ______ can retake it with STL approval, and further ________ require recommendations and support from various authorities before retesting. __________ must also be qualified on related field jobs prior to Post-CRO Fundamentals training, during which they are mentored and undergo unit-specific console training using simulators. Successful completion of ________ is verified through the LMS system, and all documentation must be properly filed. ____ must maintain qualifications by working shifts or completing simulator sessions each quarter, and they are required to pass refresher exams every three years with at least an 80% score. Failure to meet training or exam requirements is addressed by a Board of Review to determine corrective actions.\n",
      "\n",
      "Answers:\n",
      "{'outlined': {'outlined': 1.0},\n",
      " 'course': {'course': 0.17, '.': 0.16, '': 0.11},\n",
      " 'failures': {'failures': 1.0},\n",
      " 'Candidates': {'candidates': 1.0},\n",
      " 'training': {'training': 0.92},\n",
      " 'CROs': {'cros': 0.94}}\n",
      "================================================================================\n",
      "appendices\n",
      "================================================================================\n",
      "Cloze text:\n",
      "_________ being generated...\n",
      "\n",
      "Answers:\n",
      "{'Currently': {'currently': 0.31, '': 0.12}}\n",
      "================================================================================\n",
      "7-other-training\n",
      "================================================================================\n",
      "Cloze text:\n",
      "The outlined training ____________ for operations personnel in a Refinery Business Unit emphasize the necessary qualifications for working in a new plant. The Capital Project Team is tasked with developing and __________ training materials, with Learning & Development (L&D) personnel providing subject matter expertise as needed. Additionally, training _______ to plant changes and procedure updates is _______ through the Management of Change (MOC) system and the Document Management System. _______ matter experts develop and deliver this training, ensuring operators are informed of changes impacting their roles. __________ of training is documented using various electronic tools.\n",
      "\n",
      "Answers:\n",
      "{'requirements': {'requirements': 1.0},\n",
      " 'delivering': {'delivering': 1.0},\n",
      " 'related': {'related': 1.0},\n",
      " 'managed': {'managed': 1.0},\n",
      " 'Subject': {'subject': 0.93},\n",
      " 'Completion': {'completion': 0.62}}\n",
      "================================================================================\n",
      "9-unit-school-instructors-and-otj-trainers\n",
      "================================================================================\n",
      "Cloze text:\n",
      "The selection process for Unit School Instructors ____ to ensure qualified individuals are chosen to deliver crucial training for developing competent _________. Candidates must be approved by the STL, demonstrate an interest in instruction, and have completed a Train-The-Trainer class. They should have at least one year of experience at the FQO level, effective communication ______, and knowledge of safety and process protocols. Basic computer skills and ___________ with refinery intranet resources are also required. ___________ work with Unit Field Trainers to update training materials and maintain records. The UFT maintains an Instructor Directory to help STLs select eligible trainers for conducting shadow training. During this phase, ________ are guided by qualified OTJ trainers and use various resources to facilitate learning, including the Break-in Guide and Electronic Operating Manual.\n",
      "\n",
      "Answers:\n",
      "{'aims': {'aims': 0.52, 'oughtis': 0.18, 'designed': 0.09},\n",
      " 'operators': {'operatives': 0.97},\n",
      " 'skills': {'skills': 1.0},\n",
      " 'familiarity': {'familiarity': 1.0},\n",
      " 'Instructors': {'instructors': 1.0},\n",
      " 'trainees': {'trainees': 1.0}}\n",
      "================================================================================\n",
      "7-a-model-of-scientific-research-in-psychology\n",
      "================================================================================\n",
      "Cloze text:\n",
      "The text outlines a model of scientific ________ in psychology, exemplified by studies conducted by ___________ like Mehl and David Strayer. Mehl's team explored gender ___________ in talkativeness, prompted by societal stereotypes and literature gaps, leading to an empirical study that found minimal ___________ and spurred further research questions. Concurrently, Strayer's work in cognitive neuroscience examined the impact of cell phone use on driving, revealing significant ____________ and influencing policy changes. His research demonstrated the dangers of both handheld and hands-free devices, likening texting while driving to driving under the influence. These studies highlight the cyclical nature of scientific inquiry, where new research both addresses existing questions and generates new ones, contributing to a broader understanding of human behavior and _________ public policy.\n",
      "\n",
      "Answers:\n",
      "{'research': {'research': 1.0},\n",
      " 'researchers': {'researchers': 1.0},\n",
      " 'differences': {'differences': 1.0},\n",
      " 'distractions': {'distributions': 1.0},\n",
      " 'informing': {'informing': 1.0}}\n",
      "================================================================================\n",
      "18-7-the-international-assignment\n",
      "================================================================================\n",
      "Cloze text:\n",
      "Embarking on an international assignment, whether for work or study, entails navigating a complex landscape of emotional and cultural challenges. Initially marked by intrigue and excitement, expatriates often face culture _____ and a period of adjustment before embracing their host culture. This journey necessitates meticulous preparation akin to other significant life changes, emphasizing the importance of adaptability, language proficiency, and cultural understanding. Successful ___________ are those who, rather than succumbing to frustration, leverage these experiences to enhance their personal and professional growth. The process of acculturation involves various emotional stages, including initial elation, _______ shock, and eventual acceptance, followed by the challenges of reentry into one's native culture. _______ the potential for early termination of assignments due to family or ________ issues, careful consideration and preparation can mitigate these risks, making _____________ experience a valuable asset both personally and professionally.\n",
      "\n",
      "Answers:\n",
      "{'shock': {'shock': 0.96},\n",
      " 'expatriates': {'expatriates': 1.0},\n",
      " 'culture': {'culture': 0.99},\n",
      " 'Despite': {'despite': 0.86, 'given': 0.1},\n",
      " 'personal': {'personal': 1.0},\n",
      " 'international': {'international': 0.48, 'them': 0.05}}\n",
      "================================================================================\n",
      "18-3-common-cultural-characteristics\n",
      "================================================================================\n",
      "Cloze text:\n",
      "The text explores the _________ and evolution of cultures within educational, professional, and broader ________ contexts. It highlights how individuals transition from outsiders to full members through rituals and rites of initiation, which can be informal, like joining _________ for lunch, or formal, like religious ___________. In business, culture is crucial as communication fosters community, with symbolic elements such as office spaces or attire indicating status and power. Cultures also employ unique vocabularies and _______, which can both bind and constrain groups, emphasizing that adaptability is vital as cultural practices and norms evolve over time. The dynamic nature of cultures reflects the constant change in _____________ contexts and societal structures.\n",
      "\n",
      "Answers:\n",
      "{'formation': {'formation': 1.0},\n",
      " 'societal': {'societal': 1.0},\n",
      " 'coworkers': {'cafeterias': 1.0},\n",
      " 'ordinations': {'ordination': 0.98},\n",
      " 'rituals': {'rituals': 1.0},\n",
      " 'communication': {'communication': 0.58}}\n",
      "================================================================================\n",
      "19-1-what-is-a-group\n",
      "================================================================================\n",
      "Cloze text:\n",
      "_________ being generated...\n",
      "\n",
      "Answers:\n",
      "{'Currently': {'currently': 0.31, '': 0.12}}\n",
      "================================================================================\n",
      "8-interns\n",
      "================================================================================\n",
      "Cloze text:\n",
      "The intern assignment at Chevron spans one semester, or 12-14 weeks, during which operations interns are integrated with a specific crew to gain insights into the _____________ environment. Interns are expected to adhere to the same standards and ________ as full-time employees but are not considered qualified operators. Their tasks are limited and supervised by _________ operators, and they cannot respond to emergencies. Successful interns may transition to full-____ operator trainees, contingent on meeting program ____________, business needs, and HR approval. The O&M Internship Guide outlines the transition process, and an early promotion may be possible if _________ retention and performance are satisfactory.\n",
      "\n",
      "Answers:\n",
      "{'manufacturing': {'manufacturing': 1.0},\n",
      " 'policies': {'practices': 1.0},\n",
      " 'qualified': {'qualified': 0.96},\n",
      " 'time': {'time': 0.99},\n",
      " 'requirements': {'requirements': 1.0},\n",
      " 'knowledge': {'knowledge': 0.68}}\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "page_summaries = {}\n",
    "with open(\"../data/strapi-page-summaries.json\") as f:\n",
    "    for page in json.load(f):\n",
    "        if page[\"PageSummary\"]:\n",
    "            # print(page[\"PageSummary\"])\n",
    "            # page_summaries[page[\"Slug\"]] = page[\"PageSummary\"]\n",
    "            print(page[\"Slug\"])\n",
    "            print(\"=\"*80)\n",
    "            cloze_text, answers, preds = generator.generate_cloze(page[\"PageSummary\"], num_blanks=6)\n",
    "            print(\"Cloze text:\")\n",
    "            print(cloze_text)\n",
    "            print(\"\\nAnswers:\")\n",
    "            answer_dict = {answer: {word: round(prob,2) for word, prob in pred.items()} for answer, pred in zip(answers, preds)}\n",
    "            pp(answer_dict)\n",
    "            print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a5827-4f91-4fef-bf8f-e162ed857c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf]",
   "language": "python",
   "name": "conda-env-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
