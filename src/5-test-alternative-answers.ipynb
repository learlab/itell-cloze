{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef035409-0d4f-4085-883a-42523e07e2a1",
   "metadata": {},
   "source": [
    "# Lemma Constraint\n",
    "\n",
    "Make sure that gaps are only generated for lemmas that appear in the page text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed39f237-3bfa-40d0-bfbd-b94eacfac367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c821d31-0ea8-491d-9939-8880f5d0c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LH: Please don't leave installation outputs in the notebook\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe8bb68-f6ef-473b-8312-8d8009802517",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Combine the human annotations with the generated cloze exercises and source texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a2e34bf-5070-4a1a-bea7-5ddac9e1524b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>page</th>\n",
       "      <th>summary</th>\n",
       "      <th>markdown</th>\n",
       "      <th>text</th>\n",
       "      <th>contextuality</th>\n",
       "      <th>contextuality_plus</th>\n",
       "      <th>keyword</th>\n",
       "      <th>passageId</th>\n",
       "      <th>id</th>\n",
       "      <th>...</th>\n",
       "      <th>method</th>\n",
       "      <th>score</th>\n",
       "      <th>timeSpent</th>\n",
       "      <th>answers</th>\n",
       "      <th>correctAnswers</th>\n",
       "      <th>annotations</th>\n",
       "      <th>holisticScore</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>annotation_counts</th>\n",
       "      <th>pct_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>research-methods-in-psychology</td>\n",
       "      <td>1-methods-of-knowing</td>\n",
       "      <td>The text discusses various methods of acquirin...</td>\n",
       "      <td>&lt;i-callout variant=\"info\" title=\"Learning Obje...</td>\n",
       "      <td>Learning Objectives\\n\\n1. Describe the 5 metho...</td>\n",
       "      <td>{'text': 'The text discusses various methods o...</td>\n",
       "      <td>{'text': 'The text discusses various methods o...</td>\n",
       "      <td>{'text': 'The text discusses various methods o...</td>\n",
       "      <td>5</td>\n",
       "      <td>rP84qtlrKnS8VBr74Ty3</td>\n",
       "      <td>...</td>\n",
       "      <td>contextuality_plus</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>53758.247</td>\n",
       "      <td>[instincts, involves, prove, incorrect, based,...</td>\n",
       "      <td>[instincts, involves, shows, incorrect, based,...</td>\n",
       "      <td>[sentence, sentence, sentence, sentence, sente...</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-05-08T19:05:33.444Z</td>\n",
       "      <td>{'sentence': 7, 'source': 2}</td>\n",
       "      <td>77.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>research-methods-in-psychology</td>\n",
       "      <td>1-methods-of-knowing</td>\n",
       "      <td>The text discusses various methods of acquirin...</td>\n",
       "      <td>&lt;i-callout variant=\"info\" title=\"Learning Obje...</td>\n",
       "      <td>Learning Objectives\\n\\n1. Describe the 5 metho...</td>\n",
       "      <td>{'text': 'The text discusses various methods o...</td>\n",
       "      <td>{'text': 'The text discusses various methods o...</td>\n",
       "      <td>{'text': 'The text discusses various methods o...</td>\n",
       "      <td>5</td>\n",
       "      <td>BPIUqe5Fhb6faWKOo6XJ</td>\n",
       "      <td>...</td>\n",
       "      <td>contextuality_plus</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>438.322</td>\n",
       "      <td>[instincts, involves, shows, incorrect, based,...</td>\n",
       "      <td>[instincts, involves, shows, incorrect, based,...</td>\n",
       "      <td>[source, sentence, sentence, sentence, sentenc...</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-05-12T13:59:49.106Z</td>\n",
       "      <td>{'source': 3, 'sentence': 5, 'unpredictable': 1}</td>\n",
       "      <td>77.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            volume                  page  \\\n",
       "24  research-methods-in-psychology  1-methods-of-knowing   \n",
       "29  research-methods-in-psychology  1-methods-of-knowing   \n",
       "\n",
       "                                              summary  \\\n",
       "24  The text discusses various methods of acquirin...   \n",
       "29  The text discusses various methods of acquirin...   \n",
       "\n",
       "                                             markdown  \\\n",
       "24  <i-callout variant=\"info\" title=\"Learning Obje...   \n",
       "29  <i-callout variant=\"info\" title=\"Learning Obje...   \n",
       "\n",
       "                                                 text  \\\n",
       "24  Learning Objectives\\n\\n1. Describe the 5 metho...   \n",
       "29  Learning Objectives\\n\\n1. Describe the 5 metho...   \n",
       "\n",
       "                                        contextuality  \\\n",
       "24  {'text': 'The text discusses various methods o...   \n",
       "29  {'text': 'The text discusses various methods o...   \n",
       "\n",
       "                                   contextuality_plus  \\\n",
       "24  {'text': 'The text discusses various methods o...   \n",
       "29  {'text': 'The text discusses various methods o...   \n",
       "\n",
       "                                              keyword  passageId  \\\n",
       "24  {'text': 'The text discusses various methods o...          5   \n",
       "29  {'text': 'The text discusses various methods o...          5   \n",
       "\n",
       "                      id  ...              method      score  timeSpent  \\\n",
       "24  rP84qtlrKnS8VBr74Ty3  ...  contextuality_plus  77.777778  53758.247   \n",
       "29  BPIUqe5Fhb6faWKOo6XJ  ...  contextuality_plus  77.777778    438.322   \n",
       "\n",
       "                                              answers  \\\n",
       "24  [instincts, involves, prove, incorrect, based,...   \n",
       "29  [instincts, involves, shows, incorrect, based,...   \n",
       "\n",
       "                                       correctAnswers  \\\n",
       "24  [instincts, involves, shows, incorrect, based,...   \n",
       "29  [instincts, involves, shows, incorrect, based,...   \n",
       "\n",
       "                                          annotations holisticScore  \\\n",
       "24  [sentence, sentence, sentence, sentence, sente...             4   \n",
       "29  [source, sentence, sentence, sentence, sentenc...             3   \n",
       "\n",
       "                   timestamp  \\\n",
       "24  2025-05-08T19:05:33.444Z   \n",
       "29  2025-05-12T13:59:49.106Z   \n",
       "\n",
       "                                   annotation_counts pct_correct  \n",
       "24                      {'sentence': 7, 'source': 2}   77.777778  \n",
       "29  {'source': 3, 'sentence': 5, 'unpredictable': 1}   77.777778  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_df = (\n",
    "    # Human responses and ratings to a selection of Cloze exercises\n",
    "    pd.read_csv(\"../data/testResults_from_2025-05-07.csv\")\n",
    "    # Convert dictionaries to lists\n",
    "    .assign(\n",
    "        answers=lambda x: x[\"answers\"].apply(\n",
    "            lambda row: list(json.loads(row).values())\n",
    "        ),\n",
    "        correctAnswers=lambda x: x[\"correctAnswers\"].apply(\n",
    "            lambda row: list(json.loads(row).values())\n",
    "        ),\n",
    "        annotation_counts=lambda x: x[\"annotations\"].apply(\n",
    "            lambda row: Counter(json.loads(row).values())\n",
    "        ),\n",
    "        annotations=lambda x: x[\"annotations\"].apply(\n",
    "            lambda row: list(json.loads(row).values())\n",
    "        ),\n",
    "    )\n",
    "    # Calculate percentage correct\n",
    "    .assign(\n",
    "        pct_correct=lambda x: x.apply(\n",
    "            lambda row: sum(\n",
    "                a == c for a, c in zip(row[\"answers\"], row[\"correctAnswers\"])\n",
    "            )\n",
    "            / len(row[\"answers\"])\n",
    "            * 100,\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "cloze_df = pd.read_json(\n",
    "    \"../results/cloze_exercises_kl_divergence.jsonl\", lines=True\n",
    ").assign(passageId=lambda x: x.index + 1)\n",
    "\n",
    "df = pd.merge(cloze_df, annotations_df, on=\"passageId\").query(\n",
    "    'method == \"contextuality_plus\"'\n",
    ")\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9949fa8a-d32a-461f-82cf-2111ff827313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'The necessity of a scientific approach in psychology is often questioned, with many relying on common sense or intuition—known as folk psychology—for understanding human behavior. However, scientific research frequently contradicts these _________ beliefs, revealing inaccuracies. For instance, the belief that expressing anger can _________ it has been debunked, as has the notion that false ___________ are rare. Common myths, such as using only 10% of our brain or the effectiveness of _______-reducing diets, persist due to heuristics and confirmation bias. Psychologists emphasize __________ and the pursuit of empirical evidence to _________ these misconceptions. Additionally, they embrace ___________, welcoming unanswered questions as opportunities for scientific ___________.', 'gaps': [['intuitive', 238, 9], ['alleviate', 332, 9], ['confessions', 393, 11], ['calorie', 489, 7], ['skepticism', 586, 10], ['challenge', 638, 9], ['uncertainty', 697, 11], ['exploration', 773, 11]]}\n"
     ]
    }
   ],
   "source": [
    "print(df.contextuality_plus.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164acfc-8983-44ba-b579-063f95027fba",
   "metadata": {},
   "source": [
    "# Test Restricted Generation\n",
    "\n",
    "Only choose gaps whose lemmatized form appears in the source text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ba1b20-275f-462a-8e0a-42e87fb97ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualityGapper:\n",
    "    def __init__(self, model_name: str = \"answerdotai/ModernBERT-large\"):\n",
    "        # Load SpaCy for sentence splitting and preprocessing\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.min_blank_distance = 7  # Minimum distance between blanks\n",
    "\n",
    "        # Minimum log-predictability of alternatives\n",
    "        self.min_predictability = np.log(0.05)\n",
    "\n",
    "        # Part-of-Speech Blacklist (do not delete these words)\n",
    "        self.blacklist = [\n",
    "            \"PROPN\",  # Proper nouns\n",
    "            \"NUM\",  # Numbers\n",
    "            \"PUNCT\",  # Punctuation\n",
    "            \"SYM\",  # Symbols\n",
    "            \"X\",  # Other\n",
    "        ]\n",
    "\n",
    "    def _get_leading_ws_tokens(self, doc: Doc) -> list[str]:\n",
    "        \"\"\"The ModernBERT Tokenizer will work fine if we give it tokens with leading spaces.\n",
    "        SpaCy normally handles whitespace in terms of trailing space.\"\"\"\n",
    "        if not len(doc):\n",
    "            return [\"\"]\n",
    "\n",
    "        tokens = [doc[0].text]\n",
    "        # For tokens after the 0th, prepend trailing whitespace from the previous token.\n",
    "        tokens += [doc[i - 1].whitespace_ + doc[i].text for i in range(1, len(doc))]\n",
    "        return tokens\n",
    "\n",
    "    def get_token_mappings(self, tokens: list[str]) -> dict[int, list[int]]:\n",
    "        \"\"\"Get mappings between word positions and token positions\"\"\"\n",
    "        # Tokenize while keeping track of word IDs\n",
    "        tokenized = self.tokenizer(\n",
    "            tokens, return_tensors=\"pt\", is_split_into_words=True\n",
    "        )\n",
    "        word_ids = tokenized.word_ids()\n",
    "\n",
    "        # Create mapping from word position to token positions\n",
    "        word_to_tokens = defaultdict(list)\n",
    "\n",
    "        for token_idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx is not None:\n",
    "                word_to_tokens[word_idx].append(token_idx)\n",
    "\n",
    "        return word_to_tokens\n",
    "\n",
    "    def get_masked_logits(\n",
    "        self, tokens: list[str], mask_idx: int\n",
    "    ) -> tuple[torch.Tensor, int]:\n",
    "        \"\"\"Get model logits for a masked position in text\"\"\"\n",
    "        # Get the word tokens and their alignment info\n",
    "        word_to_tokens = self.get_token_mappings(tokens)\n",
    "\n",
    "        # Find all token positions for the word we want to mask\n",
    "        token_positions = word_to_tokens[mask_idx]\n",
    "\n",
    "        # Create masked version of the text\n",
    "        input_ids = self.tokenizer(\n",
    "            tokens, is_split_into_words=True, return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "        masked_ids = input_ids.clone()\n",
    "\n",
    "        # ID of the first subword token that we masked\n",
    "        first_token_id = input_ids[token_positions[0]]\n",
    "\n",
    "        # Mask all tokens corresponding to our target word\n",
    "        masked_ids[token_positions] = self.tokenizer.mask_token_id\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = self.model(input_ids.unsqueeze(0).to(self.device))\n",
    "\n",
    "        # Get logits\n",
    "        logits = outputs.logits[0, token_positions, :]\n",
    "\n",
    "        return logits, first_token_id\n",
    "\n",
    "    def get_contextuality_score(\n",
    "        self,\n",
    "        page_doc: Doc,\n",
    "        summary_doc: Doc,\n",
    "        sent: Span,\n",
    "        tok: Token,\n",
    "        method: str = \"kl\",\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate contextuality score for a word position using full page context\n",
    "\n",
    "        Args:\n",
    "            page_doc: The full page text as a spaCy Doc\n",
    "            summary_doc: The summary text as a spaCy Doc\n",
    "            sent: The sentence from the summary containing the token\n",
    "            tok: The token from the summary to evaluate\n",
    "            method: \"kl\" for kl-divergence or \"contextuality\" for contextuality score\n",
    "\n",
    "        Returns:\n",
    "            Contextuality score\n",
    "        \"\"\"\n",
    "\n",
    "        # Get logits for both full text and sentence text\n",
    "        # For the full text context, we use the page + summary\n",
    "        full_toks = self._get_leading_ws_tokens(page_doc) + self._get_leading_ws_tokens(\n",
    "            summary_doc\n",
    "        )\n",
    "        full_pos = len(page_doc) + tok.i  # Position of token in full document\n",
    "        full_logits, word_id = self.get_masked_logits(full_toks, full_pos)\n",
    "\n",
    "        # For the local context, we use just the sentence from the summary\n",
    "        sent_pos = tok.i - sent.start  # Position of token in the sentence\n",
    "        sent_logits, _ = self.get_masked_logits([tok.text for tok in sent], sent_pos)\n",
    "\n",
    "        # Calculate probabilities using first sub-word token\n",
    "        full_probs = torch.softmax(full_logits[0], dim=0)\n",
    "        sent_probs = torch.softmax(sent_logits[0], dim=0)\n",
    "\n",
    "        p = full_probs[word_id]\n",
    "        q = sent_probs[word_id]\n",
    "\n",
    "        if method == \"kl\":\n",
    "            # KL-divergence is p*log(p/q)\n",
    "            score = float(p * torch.log2(p / q))\n",
    "        elif method == \"contextuality\":\n",
    "            # Contextuality is distance between full-text and sentence probability\n",
    "            score = float(p - q)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown method.\")\n",
    "\n",
    "        return score\n",
    "\n",
    "    def choose_blank_positions(\n",
    "        self, page_doc: Doc, summary_doc: Doc, num_blanks: int\n",
    "    ) -> list[int]:\n",
    "        \"\"\"Choose positions to blank in the summary based on contextuality scores with full page\"\"\"\n",
    "        scores = []\n",
    "        valid_positions = []\n",
    "\n",
    "        page_lemmas = {tok.lemma_ for tok in page_doc}\n",
    "\n",
    "        # Calculate scores for each position in the summary\n",
    "        for i, sent in enumerate(summary_doc.sents):\n",
    "            if i == 0:\n",
    "                continue  # Skip first sentence\n",
    "            for tok in sent:\n",
    "                if (\n",
    "                    len(tok.text) < 3\n",
    "                    or tok.pos_ in self.blacklist\n",
    "                    or tok.is_stop\n",
    "                    or not tok.text.isalpha()\n",
    "                    or tok.lemma_ not in page_lemmas\n",
    "                ):\n",
    "                    scores.append(-float(\"inf\"))\n",
    "                else:\n",
    "                    # Calculate contextuality using both the full page and summary\n",
    "                    score = self.get_contextuality_score(\n",
    "                        page_doc, summary_doc, sent, tok\n",
    "                    )\n",
    "                    scores.append(score)\n",
    "                valid_positions.append(tok.i)\n",
    "\n",
    "        # Convert to numpy for easier manipulation\n",
    "        scores = np.array(scores)\n",
    "\n",
    "        # Choose positions greedily while maintaining minimum distance\n",
    "        positions = []\n",
    "        for _ in range(num_blanks):\n",
    "            if np.all(scores == -float(\"inf\")):\n",
    "                break\n",
    "\n",
    "            # Choose highest scoring position\n",
    "            idx = np.argmax(scores)\n",
    "            pos = valid_positions[idx]\n",
    "            positions.append(pos)\n",
    "\n",
    "            # Zero out scores within minimum distance\n",
    "            start = max(0, idx - self.min_blank_distance)\n",
    "            end = min(len(scores), idx + self.min_blank_distance + 1)\n",
    "            scores[start:end] = -float(\"inf\")\n",
    "\n",
    "        return sorted(positions)\n",
    "\n",
    "    def get_alternates(self, tokens: list[str], topk=5) -> list[dict]:\n",
    "        \"\"\"Get top k predictions for the masked positions in tokens\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries, one per masked position, with candidate words and their probabilities\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        # Find all mask positions\n",
    "        mask_positions = [i for i, token in enumerate(tokens) if token == \"[MASK]\"]\n",
    "\n",
    "        for mask_pos in mask_positions:\n",
    "            word_candidates = {}\n",
    "\n",
    "            # Try different mask lengths (1, 2, or 3 tokens)\n",
    "            for mask_length in range(1, 4):\n",
    "                # Replace the single mask with multiple if needed\n",
    "                masked_tokens = (\n",
    "                    tokens[:mask_pos]\n",
    "                    + [\"[MASK]\"] * mask_length\n",
    "                    + tokens[mask_pos + 1 :]\n",
    "                )\n",
    "\n",
    "                # Get initial predictions for first token\n",
    "                current_candidates = []\n",
    "                logits, _ = self.get_masked_logits(masked_tokens, mask_pos)\n",
    "                probs = torch.softmax(logits[0], dim=0)\n",
    "                top_values, top_indices = torch.topk(probs, topk)\n",
    "\n",
    "                # Start with first token candidates\n",
    "                for idx, prob in zip(top_indices.tolist(), top_values.tolist()):\n",
    "                    current_candidates.append(([idx], prob))\n",
    "\n",
    "                # Build up multi-token predictions if needed\n",
    "                for token_idx in range(1, mask_length):\n",
    "                    new_candidates = []\n",
    "                    for token_ids, prob in current_candidates:\n",
    "                        # Fill in what we've predicted so far\n",
    "                        partial_filled = tokens.copy()\n",
    "                        filled_text = self.tokenizer.decode(token_ids)\n",
    "                        remaining_masks = mask_length - token_idx\n",
    "\n",
    "                        partial_filled = (\n",
    "                            tokens[:mask_pos]\n",
    "                            + [filled_text]\n",
    "                            + [\"[MASK]\"] * remaining_masks\n",
    "                            + tokens[mask_pos + 1 :]\n",
    "                        )\n",
    "\n",
    "                        # Get prediction for next position\n",
    "                        next_logits, _ = self.get_masked_logits(\n",
    "                            partial_filled, mask_pos + 1\n",
    "                        )\n",
    "                        next_probs = torch.softmax(next_logits[0], dim=0)\n",
    "                        next_values, next_indices = torch.topk(next_probs, 1)\n",
    "\n",
    "                        # Add to candidates\n",
    "                        new_token_ids = token_ids + [next_indices[0].item()]\n",
    "                        new_prob = prob * next_values[0].item()\n",
    "                        new_candidates.append((new_token_ids, new_prob))\n",
    "\n",
    "                    current_candidates = new_candidates\n",
    "\n",
    "                # Add final decoded words\n",
    "                for token_ids, prob in current_candidates:\n",
    "                    word = self.tokenizer.decode(token_ids).strip()\n",
    "                    if \" \" in word:\n",
    "                        # Word contains a space (is actually multiple words)\n",
    "                        continue\n",
    "                    if word not in word_candidates or prob > word_candidates[word]:\n",
    "                        word_candidates[word] = prob\n",
    "\n",
    "            # Sort candidates by probability\n",
    "            sorted_candidates = sorted(\n",
    "                word_candidates.items(), key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "            predictions.append({word: prob for word, prob in sorted_candidates[:topk]})\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def generate_cloze(\n",
    "        self,\n",
    "        summary_text: str,\n",
    "        page_text: str = \"\",\n",
    "        num_blanks: int = 10,\n",
    "    ) -> tuple[str, list[str], list[dict[str, float]]]:\n",
    "        \"\"\"Generate a cloze text from summary using page for context\n",
    "\n",
    "        Args:\n",
    "            page_text: The full page text\n",
    "            summary_text: The summary text to create gaps in\n",
    "            num_blanks: Number of blanks to create\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (cloze_text, answers, alternates)\n",
    "        \"\"\"\n",
    "        # Process both texts\n",
    "        page_doc = self.nlp(page_text)\n",
    "        summary_doc = self.nlp(summary_text)\n",
    "\n",
    "        # Choose positions to blank in the summary\n",
    "        masked_positions = self.choose_blank_positions(\n",
    "            page_doc, summary_doc, num_blanks\n",
    "        )\n",
    "\n",
    "        # Get the answers (the original words that will be blanked)\n",
    "        answers = [summary_doc[pos].text for pos in masked_positions]\n",
    "\n",
    "        # Replace tokens with mask\n",
    "        summary_tokens = np.array(self._get_leading_ws_tokens(summary_doc))\n",
    "        summary_tokens[masked_positions] = \"[MASK]\"\n",
    "        summary_tokens = summary_tokens.tolist()\n",
    "\n",
    "        # Construct cloze token input for gap predictions\n",
    "        cloze_tokens = self._get_leading_ws_tokens(page_doc) + summary_tokens\n",
    "\n",
    "        # Collect gaps\n",
    "        gaps = []\n",
    "        for tok in summary_doc:\n",
    "            if tok.i in masked_positions:\n",
    "                gaps.append((tok.text, tok.idx, len(tok.text)))\n",
    "\n",
    "        return gaps\n",
    "\n",
    "\n",
    "    def score_cloze_answers(self, text_with_masks: str, top_k: int) -> list[dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Return, for each [MASK], a dict {token -> probability}.\n",
    "        \"\"\"\n",
    "        # Tokenize to the right device\n",
    "        inputs = self.tokenizer(text_with_masks, return_tensors=\"pt\").to(self.device)\n",
    "    \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        logits = outputs.logits  # [batch, seq, vocab]\n",
    "    \n",
    "        # Locate [MASK] tokens\n",
    "        mask_positions = torch.where(inputs[\"input_ids\"][0] == self.tokenizer.mask_token_id)[0]\n",
    "    \n",
    "        # Collect top-k predictions per mask\n",
    "        results = []\n",
    "        for pos in mask_positions:\n",
    "            vec = logits[0, pos.item()]              # [vocab]\n",
    "            probs = torch.softmax(vec, dim=-1)       # probabilities\n",
    "            top_probs, top_idx = torch.topk(probs, top_k)\n",
    "            preds = {}\n",
    "            for p, idx in zip(top_probs.tolist(), top_idx.tolist()):\n",
    "                token = self.tokenizer.decode([idx]).strip()\n",
    "                preds[token] = p\n",
    "            results.append(preds)\n",
    "        return results # list of dicts with each word's prob distribution of top k accepted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76c535a-accd-44d7-97af-52dcbe35499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LH: Use doc strings for function descriptions\n",
    "# LH: The underscore before a function name indicates that it is for internal use only, usually methods in a class that shouldn't be called from outside that class.\n",
    "# LH: I wouldn't use it for functions inside the global scope of a notebook.\n",
    "\n",
    "def _mask_by_char_spans(text: str, spans: list[tuple[int,int]]) -> str:  # converting character positions from df to masks\n",
    "    parts, prev = [], 0\n",
    "    for (start, end) in spans:\n",
    "        parts.append(text[prev:start])\n",
    "        parts.append(\"[MASK]\")\n",
    "        prev = end\n",
    "    parts.append(text[prev:])\n",
    "    return \"\".join(parts)\n",
    "\n",
    "# LH: Could do this a bit more cleanly like\n",
    "def mask_by_char_spans(text: str, spans: list[tuple[int, int]]) -> str:\n",
    "    \"\"\"Replace text at gapped locations with [MASK] using (start, end) tuples \n",
    "    \"\"\"\n",
    "    for start, end in sorted(spans, reverse=True):\n",
    "        text = text[:start] + \"[MASK]\" + text[end:]\n",
    "    return text\n",
    "\n",
    "def _extract_from_contextuality_plus(row): # extract all relevant information from dataframe\n",
    "    \"\"\"\n",
    "    Return:\n",
    "      summary_text : str\n",
    "      gap_spans    : list[(start, end)]\n",
    "      originals    : list[str]\n",
    "    \"\"\"\n",
    "    cp = row.contextuality_plus\n",
    "    summary_text = cp.get(\"text\", row.summary)\n",
    "    gap_spans, originals = [], []\n",
    "    for word, start, length in cp.get(\"gaps\", []):\n",
    "        originals.append(word)\n",
    "        gap_spans.append((start, start + length))\n",
    "    return summary_text, gap_spans, originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ed305c-cb3f-4f62-ad0b-ca6b0d48dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_accepted_answers(\n",
    "    gapper,\n",
    "    page_text: str,\n",
    "    row,                          # pass the whole row now\n",
    "    min_probability: float = 0.05,\n",
    "    topk: int | None = None,\n",
    "):\n",
    "    # exact cloze text + spans humans saw\n",
    "    summary_text, gap_spans, originals = _extract_from_contextuality_plus(row)\n",
    "\n",
    "    # masked summary by character spans\n",
    "    masked_summary = mask_by_char_spans(summary_text, gap_spans)\n",
    "\n",
    "    # full context = page + masked summary\n",
    "    page_str = \"\".join(gapper._get_leading_ws_tokens(gapper.nlp(page_text)))\n",
    "    full_context = page_str + \"\\n\\n\" + masked_summary\n",
    "\n",
    "    # score each [MASK]\n",
    "    preds_per_gap = gapper.score_cloze_answers(full_context, top_k=(topk or 5000))\n",
    "\n",
    "    # build acceptable sets\n",
    "    accepted = []\n",
    "    for i, pred_dict in enumerate(preds_per_gap):\n",
    "        keep = set()\n",
    "        for w, p in pred_dict.items():\n",
    "            w2 = w.strip()\n",
    "            if w2 and \" \" not in w2 and w2.isalpha() and p >= min_probability:\n",
    "                keep.add(w2.lower())\n",
    "        if i < len(originals):  # always include the \"correct\" word\n",
    "            keep.add(originals[i].strip().lower())\n",
    "        accepted.append(keep)\n",
    "\n",
    "    return accepted, originals\n",
    "\n",
    "# Evaluate accuracy of annotations given new acceptable answers\n",
    "def rescore_annotations(\n",
    "    df,\n",
    "    gapper,\n",
    "    min_probability: float = 0.05,\n",
    "    topk: int | None = None,\n",
    "):\n",
    "    # LH: Put imports at the top of the notebook.\n",
    "    # These are already imported...\n",
    "    import pandas as pd\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    rows = []\n",
    "    for row in tqdm(df.itertuples(), total=len(df), desc=\"Rescoring\"):\n",
    "        accepted, originals = generate_accepted_answers(\n",
    "            gapper=gapper,  # LH: You wouldn't need to pass this class instance around if this function were a class method.\n",
    "            page_text=row.text,  \n",
    "            row=row,\n",
    "            min_probability=min_probability,\n",
    "            topk=topk,        \n",
    "        )\n",
    "\n",
    "        human = [annotation.strip().lower() for annotation in row.annotations]\n",
    "        flags = []\n",
    "        for i, ans in enumerate(human):\n",
    "            okset = accepted[i] if i < len(accepted) else set()\n",
    "            if not okset and i < len(originals):           # safety fallback\n",
    "                okset = {originals[i].strip().lower()}\n",
    "            flags.append(ans in okset)\n",
    "\n",
    "        percent = 100.0 * (sum(flags) / max(1, len(flags)))\n",
    "        rows.append({\n",
    "            \"accepted_answers\": accepted,\n",
    "            \"human_correctness\": flags,\n",
    "            \"percent_correct_alt\": percent,\n",
    "            \"alt_counts\": [len(s) for s in accepted],\n",
    "        })\n",
    "\n",
    "    extra = pd.DataFrame(rows, index=df.index)\n",
    "    return pd.concat([df.copy(), extra], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a9a70-0232-4e58-aa1a-013c96734f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LH: see above\n",
    "# !pip install -U \"transformers>=4.46\" \"tokenizers>=0.15\" safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e011595b-5546-4f7a-9621-5d97a9b20fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gapper = ContextualityGapper(model_name=\"answerdotai/ModernBERT-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ec7b5-243c-4ddc-bece-59064296d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "#      ANALYZE HUMAN AGREEMENT PATTERNS\n",
    "# ============================================\n",
    "def analyze_human_agreement(df):\n",
    "    \"\"\"\n",
    "    Find cases where multiple humans gave the same *incorrect* answer\n",
    "    for the same passage and the same gap index.\n",
    "    Returns: (counts_df, common_df)\n",
    "    \"\"\"\n",
    "    # LH: See above\n",
    "    import pandas as pd\n",
    "\n",
    "    recs = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        pid = row[\"passageId\"]\n",
    "        anns = row[\"annotations\"]\n",
    "        gold = row[\"correctAnswers\"]\n",
    "        # LH: Please use descriptive variable names\n",
    "        for g, (h, c) in enumerate(zip(anns, gold)):\n",
    "            h = h.strip().lower()\n",
    "            c = c.strip().lower()\n",
    "            if h != c:\n",
    "                recs.append({\"passageId\": pid, \"gap_idx\": g, \"human_answer\": h})\n",
    "\n",
    "    if not recs:\n",
    "        print(\"No wrong answers found.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    tmp = pd.DataFrame(recs)\n",
    "    counts = (tmp\n",
    "              .value_counts([\"passageId\", \"gap_idx\", \"human_answer\"])\n",
    "              .reset_index(name=\"n\")\n",
    "              .sort_values(\"n\", ascending=False))\n",
    "\n",
    "    common = counts[counts[\"n\"] > 1]\n",
    "\n",
    "    print(f\"Found {len(common)} (passageId, gap_idx) cases with agreement on the same wrong answer.\")\n",
    "    if not common.empty:\n",
    "        print(\"\\nTop 10:\")\n",
    "        print(common.head(10).to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nNo per-gap agreements detected. This usually means you have one annotator per passage, \"\n",
    "              \"or the wrong answers are all unique per gap.\")\n",
    "\n",
    "    return counts, common\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "#            PARAMETER GRID SEARCH\n",
    "# ============================================\n",
    "\n",
    "def test_parameters(df, gapper, min_prob_values, topk_values=None):\n",
    "    \"\"\"\n",
    "    Test different parameter combinations and collect metrics.\n",
    "    \n",
    "    Args:\n",
    "        df: Your dataframe\n",
    "        gapper: Your ContextualityGapper instance\n",
    "        min_prob_values: List of min_probability values to test\n",
    "        topk_values: List of topk values to test (or None to not limit)\n",
    "    \"\"\"\n",
    "    # LH: See above\n",
    "    import pandas as pd\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # If topk_values is None, use a very large number (effectively no limit)\n",
    "    if topk_values is None:\n",
    "        # LH: A minor note, but I like to use underscores for the thousands place\n",
    "        topk_values = [10_000]  # Effectively unlimited \n",
    "    \n",
    "    # Test each combination\n",
    "    total_tests = len(min_prob_values) * len(topk_values)\n",
    "    pbar = tqdm(total=total_tests, desc=\"Testing parameters\")\n",
    "    \n",
    "    for min_prob in min_prob_values:\n",
    "        for topk in topk_values:\n",
    "            # Rescore with these parameters\n",
    "            df_rescored = rescore_annotations(\n",
    "                df.copy(), \n",
    "                gapper, \n",
    "                min_probability=min_prob,\n",
    "                topk=topk if topk != 10000 else None\n",
    "            )\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                'min_prob': min_prob,\n",
    "                'topk': topk if topk != 10000 else 'unlimited',  # LH: Leaving this as the real value is better so it always the same dtype\n",
    "                'mean_accuracy': df_rescored['percent_correct_alt'].mean(),\n",
    "                'median_accuracy': df_rescored['percent_correct_alt'].median(),\n",
    "                'std_accuracy': df_rescored['percent_correct_alt'].std(),\n",
    "                'mean_alternatives': df_rescored['alt_counts'].apply(lambda x: np.mean(x)).mean(),\n",
    "                'median_alternatives': df_rescored['alt_counts'].apply(lambda x: np.median(x)).median(),\n",
    "                'max_alternatives': df_rescored['alt_counts'].apply(lambda x: np.max(x)).max(),\n",
    "                'gaps_with_1_5_alts': sum(df_rescored['alt_counts'].apply(\n",
    "                    lambda x: all(1 <= c <= 5 for c in x)\n",
    "                )) / len(df_rescored) * 100\n",
    "            }\n",
    "            \n",
    "            results.append(metrics)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ============================================\n",
    "#                 BEST PARAMS\n",
    "# ============================================\n",
    "\n",
    "def analyze_best_parameters(df, gapper, min_prob, topk, common_df):\n",
    "    \"\"\"\n",
    "    Analysis of a specific parameter combo.\n",
    "    common_df is the DataFrame returned by analyze_human_agreement(...)[1]\n",
    "    \"\"\"\n",
    "    # LH: see above\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    df_rescored = rescore_annotations(\n",
    "        df.copy(), gapper, min_probability=min_prob, topk=topk\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DETAILED ANALYSIS: min_prob={min_prob}, topk={topk if topk else 'unlimited'}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Overall stats\n",
    "    print(f\"\\nOverall Performance:\")\n",
    "    print(f\"  Mean accuracy: {df_rescored['percent_correct_alt'].mean():.1f}%\")\n",
    "    print(f\"  Median accuracy: {df_rescored['percent_correct_alt'].median():.1f}%\")\n",
    "    print(f\"  Std deviation: {df_rescored['percent_correct_alt'].std():.1f}%\")\n",
    "\n",
    "    # Alternative counts (flatten)\n",
    "    alt_counts_flat = [c for row in df_rescored['alt_counts'] for c in row]\n",
    "    print(f\"\\nAlternative Answers per Gap:\")\n",
    "    print(f\"  Mean: {np.mean(alt_counts_flat):.1f}\")\n",
    "    print(f\"  Median: {np.median(alt_counts_flat):.0f}\")\n",
    "    print(f\"  Max: {np.max(alt_counts_flat)}\")\n",
    "    print(f\"  % gaps with 1-5 alternatives: {sum(1 <= c <= 5 for c in alt_counts_flat) / len(alt_counts_flat) * 100:.1f}%\")\n",
    "\n",
    "    # How many “common wrong” are now accepted for the same passage+gap\n",
    "    accepted_mistakes = 0\n",
    "    total_mistakes = len(common_df)\n",
    "\n",
    "    if total_mistakes:\n",
    "        # Build a quick lookup: passageId -> row in rescored df\n",
    "        rescored_by_pid = df_rescored.set_index(\"passageId\")\n",
    "\n",
    "        for _, rec in common_df.iterrows():\n",
    "            pid = rec[\"passageId\"]\n",
    "            gap = int(rec[\"gap_idx\"])\n",
    "            ans = str(rec[\"human_answer\"]).strip().lower()\n",
    "\n",
    "            if pid in rescored_by_pid.index:\n",
    "                row = rescored_by_pid.loc[pid]\n",
    "                if gap < len(row[\"accepted_answers\"]):\n",
    "                    if ans in row[\"accepted_answers\"][gap]:\n",
    "                        accepted_mistakes += 1\n",
    "\n",
    "    print(f\"\\nCommon Human Agreements (per passage+gap):\")\n",
    "    if total_mistakes:\n",
    "        print(f\"  Accepting {accepted_mistakes}/{total_mistakes} agreed-on 'incorrect' answers\")\n",
    "    else:\n",
    "        print(\"  (None found in this dataset)\")\n",
    "\n",
    "    return df_rescored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20ec33-5f78-455f-bd56-a26b8ac6e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_testing(df, gapper):\n",
    "    print(\"=\"*60)\n",
    "    print(\"PARAMETER OPTIMIZATION FOR CLOZE SCORING\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(\"\\nSTEP 1: Analyzing Human Agreement Patterns_____\")\n",
    "    counts_df, common_df = analyze_human_agreement(df)\n",
    "\n",
    "    print(\"\\nSTEP 2: Testing Parameter Combinations_____\")\n",
    "    min_prob_values = [0.01, 0.02, 0.03, 0.05, 0.07, 0.10, 0.15, 0.20]\n",
    "    topk_values = [5, 10, 20, 50, None]  # None = unlimited\n",
    "    results_df = test_parameters(df, gapper, min_prob_values, topk_values)\n",
    "\n",
    "    print(\"\\nSTEP 3: Finding Optimal Parameters_____\")\n",
    "    results_df['score'] = (\n",
    "        results_df['mean_accuracy'] * 2\n",
    "        + results_df['gaps_with_1_5_alts']\n",
    "        + (5 - abs(results_df['mean_alternatives'] - 3)) * 10\n",
    "    )\n",
    "    best_params = results_df.nlargest(5, 'score')\n",
    "    print(\"\\nTOP 5 PARAMETER COMBINATIONS:\")\n",
    "    print(best_params[['min_prob', 'topk', 'mean_accuracy',\n",
    "                       'mean_alternatives', 'gaps_with_1_5_alts', 'score']].to_string())\n",
    "\n",
    "    best_row = best_params.iloc[0]\n",
    "    best_min_prob = best_row['min_prob']\n",
    "    best_topk = None if best_row['topk'] == 'unlimited' else best_row['topk']\n",
    "\n",
    "    print(\"\\nSTEP 4: Best Parameters______\")\n",
    "    df_best = analyze_best_parameters(df, gapper, best_min_prob, best_topk, common_df)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nRECOMMENDED PARAMETERS:\")\n",
    "    print(f\"  min_probability = {best_min_prob}\")\n",
    "    print(f\"  topk = {best_topk if best_topk else 'None (unlimited)'}\")\n",
    "    print(f\"\\nExpected Performance:\")\n",
    "    print(f\"  Mean human accuracy: {best_row['mean_accuracy']:.1f}%\")\n",
    "    print(f\"  Mean alternatives per gap: {best_row['mean_alternatives']:.1f}\")\n",
    "    print(f\"  Gaps with 1-5 alternatives: {best_row['gaps_with_1_5_alts']:.1f}%\")\n",
    "\n",
    "    return results_df, df_best, best_min_prob, best_topk\n",
    "\n",
    "\n",
    "# Run the testing\n",
    "results_df, df_best, best_min_prob, best_topk = run_testing(df, gapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807ab4a-9ed0-4bba-bb02-5ce23eb03b2a",
   "metadata": {},
   "source": [
    "## Compare Gaps\n",
    "\n",
    "Without collecting additional annotations, the best we can do is look at which gaps would be retained with this method and which gaps would be removed.\n",
    "\n",
    "If this method disproportionately removes gaps that were more difficult to answer, then we can infer that the lemma overlap restriction will make the cloze exercise easier (which is what we want).\n",
    "\n",
    "First, we see that the lemma overlap constraint does not substantially decrease the number of gaps that are generated (8.65 gaps per exercise vs. 9 gaps per exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e35b07f-3b6a-406c-b053-768aa4f3c4c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    23.000000\n",
       "mean      8.652174\n",
       "std       1.112274\n",
       "min       7.000000\n",
       "25%       7.500000\n",
       "50%       9.000000\n",
       "75%       9.000000\n",
       "max      10.000000\n",
       "Name: restricted_answers, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    23.000000\n",
       "mean      9.000000\n",
       "std       0.738549\n",
       "min       8.000000\n",
       "25%       8.500000\n",
       "50%       9.000000\n",
       "75%       9.500000\n",
       "max      10.000000\n",
       "Name: correctAnswers, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.restricted_answers.str.len().describe())\n",
    "display(df.correctAnswers.str.len().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d279c1-0770-4e7b-abad-6cc564cb4b35",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The lemma overlap restriction retains 44% of gaps that are \"source-predictable\" (ideal), and removes 31% of gaps that were scored as \"unpredictable\" (bad gaps). This is a good indication that the lemma overlap constraint improves the cloze exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "686c5219-983b-4c0b-94d3-08cfb724482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained gaps\n",
      "[('passage', 0.18), ('sentence', 0.3), ('source', 0.44), ('unpredictable', 0.08)]\n",
      "\n",
      "Removed gaps\n",
      "[('passage', 0.17), ('sentence', 0.43), ('source', 0.09), ('unpredictable', 0.31)]\n"
     ]
    }
   ],
   "source": [
    "unchanged = []\n",
    "removed = []\n",
    "# added = []\n",
    "\n",
    "\n",
    "def normalize_counter(c: Counter):\n",
    "    total = sum(c.values())\n",
    "    for key in c:\n",
    "        c[key] = round(c[key] / total, 2)\n",
    "    return c\n",
    "\n",
    "\n",
    "for row in df.itertuples():\n",
    "    for answer, rating in zip(row.correctAnswers, row.annotations):\n",
    "        if answer in row.restricted_answers:\n",
    "            unchanged.append(rating)\n",
    "        else:\n",
    "            removed.append(rating)\n",
    "\n",
    "print(\"Retained gaps\")\n",
    "print(sorted(normalize_counter(Counter(unchanged)).items()))\n",
    "print(\"\\nRemoved gaps\")\n",
    "print(sorted(normalize_counter(Counter(removed)).items()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf]",
   "language": "python",
   "name": "conda-env-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
