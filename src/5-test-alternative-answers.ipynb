{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef035409-0d4f-4085-883a-42523e07e2a1",
   "metadata": {},
   "source": [
    "# Lemma Constraint\n",
    "\n",
    "Make sure that gaps are only generated for lemmas that appear in the page text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed39f237-3bfa-40d0-bfbd-b94eacfac367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d07b16-12cd-449e-be0b-5cc0788151f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy==3.7.2\n",
    "# !pip install transformers\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe8bb68-f6ef-473b-8312-8d8009802517",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Combine the human annotations with the generated cloze exercises and source texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a2e34bf-5070-4a1a-bea7-5ddac9e1524b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>page</th>\n",
       "      <th>summary</th>\n",
       "      <th>markdown</th>\n",
       "      <th>text</th>\n",
       "      <th>contextuality</th>\n",
       "      <th>contextuality_plus</th>\n",
       "      <th>keyword</th>\n",
       "      <th>passageId</th>\n",
       "      <th>id</th>\n",
       "      <th>...</th>\n",
       "      <th>method</th>\n",
       "      <th>score</th>\n",
       "      <th>timeSpent</th>\n",
       "      <th>answers</th>\n",
       "      <th>correctAnswers</th>\n",
       "      <th>annotations</th>\n",
       "      <th>holisticScore</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>annotation_counts</th>\n",
       "      <th>pct_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>research-methods-in-psychology-demo</td>\n",
       "      <td>13-drawing-conclusions-and-reporting-the-resul...</td>\n",
       "      <td>Scientific theories are continually evaluated ...</td>\n",
       "      <td>&lt;i-callout variant=\"info\" title=\"Learning Obje...</td>\n",
       "      <td>Learning Objectives\\n\\n1. Identify the conclus...</td>\n",
       "      <td>{'text': 'Scientific theories are continually ...</td>\n",
       "      <td>{'text': 'Scientific theories are continually ...</td>\n",
       "      <td>{'text': 'Scientific theories are continually ...</td>\n",
       "      <td>7</td>\n",
       "      <td>UttExqGS6dSU5PLaLe85</td>\n",
       "      <td>...</td>\n",
       "      <td>contextuality_plus</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>176.596</td>\n",
       "      <td>[confirming, theory, theories, philosophical, ...</td>\n",
       "      <td>[Confirming, theory, theories, philosophical, ...</td>\n",
       "      <td>[source, source, source, source, source, sourc...</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-05-19T19:27:19.842Z</td>\n",
       "      <td>{'source': 8, 'passage': 1, 'unpredictable': 1}</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>research-methods-in-psychology-demo</td>\n",
       "      <td>9-generating-good-research-questions-1</td>\n",
       "      <td>When developing a research idea, transforming ...</td>\n",
       "      <td>&lt;i-callout variant=\"info\" title=\"Learning Obje...</td>\n",
       "      <td>Learning Objectives\\n\\n1. Describe some techni...</td>\n",
       "      <td>{'text': 'When developing a research idea, tra...</td>\n",
       "      <td>{'text': 'When developing a research idea, tra...</td>\n",
       "      <td>{'text': 'When developing a research idea, tra...</td>\n",
       "      <td>3</td>\n",
       "      <td>3tX80XR8HgqetI32rkil</td>\n",
       "      <td>...</td>\n",
       "      <td>contextuality_plus</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>469.855</td>\n",
       "      <td>[achieved, researchers, conceptualizing, study...</td>\n",
       "      <td>[achieved, researchers, conceptualizing, explo...</td>\n",
       "      <td>[sentence, source, sentence, sentence, sentenc...</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-06-02T21:59:37.864Z</td>\n",
       "      <td>{'sentence': 4, 'source': 4, 'unpredictable': 1}</td>\n",
       "      <td>44.444444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 volume  \\\n",
       "37  research-methods-in-psychology-demo   \n",
       "16  research-methods-in-psychology-demo   \n",
       "\n",
       "                                                 page  \\\n",
       "37  13-drawing-conclusions-and-reporting-the-resul...   \n",
       "16             9-generating-good-research-questions-1   \n",
       "\n",
       "                                              summary  \\\n",
       "37  Scientific theories are continually evaluated ...   \n",
       "16  When developing a research idea, transforming ...   \n",
       "\n",
       "                                             markdown  \\\n",
       "37  <i-callout variant=\"info\" title=\"Learning Obje...   \n",
       "16  <i-callout variant=\"info\" title=\"Learning Obje...   \n",
       "\n",
       "                                                 text  \\\n",
       "37  Learning Objectives\\n\\n1. Identify the conclus...   \n",
       "16  Learning Objectives\\n\\n1. Describe some techni...   \n",
       "\n",
       "                                        contextuality  \\\n",
       "37  {'text': 'Scientific theories are continually ...   \n",
       "16  {'text': 'When developing a research idea, tra...   \n",
       "\n",
       "                                   contextuality_plus  \\\n",
       "37  {'text': 'Scientific theories are continually ...   \n",
       "16  {'text': 'When developing a research idea, tra...   \n",
       "\n",
       "                                              keyword  passageId  \\\n",
       "37  {'text': 'Scientific theories are continually ...          7   \n",
       "16  {'text': 'When developing a research idea, tra...          3   \n",
       "\n",
       "                      id  ...              method      score  timeSpent  \\\n",
       "37  UttExqGS6dSU5PLaLe85  ...  contextuality_plus  80.000000    176.596   \n",
       "16  3tX80XR8HgqetI32rkil  ...  contextuality_plus  44.444444    469.855   \n",
       "\n",
       "                                              answers  \\\n",
       "37  [confirming, theory, theories, philosophical, ...   \n",
       "16  [achieved, researchers, conceptualizing, study...   \n",
       "\n",
       "                                       correctAnswers  \\\n",
       "37  [Confirming, theory, theories, philosophical, ...   \n",
       "16  [achieved, researchers, conceptualizing, explo...   \n",
       "\n",
       "                                          annotations holisticScore  \\\n",
       "37  [source, source, source, source, source, sourc...             3   \n",
       "16  [sentence, source, sentence, sentence, sentenc...             3   \n",
       "\n",
       "                   timestamp  \\\n",
       "37  2025-05-19T19:27:19.842Z   \n",
       "16  2025-06-02T21:59:37.864Z   \n",
       "\n",
       "                                   annotation_counts pct_correct  \n",
       "37   {'source': 8, 'passage': 1, 'unpredictable': 1}   70.000000  \n",
       "16  {'sentence': 4, 'source': 4, 'unpredictable': 1}   44.444444  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_df = (\n",
    "    # Human responses and ratings to a selection of Cloze exercises\n",
    "    pd.read_csv(\"../data/testResults_from_2025-05-07.csv\")\n",
    "    # Convert dictionaries to lists\n",
    "    .assign(\n",
    "        answers=lambda x: x[\"answers\"].apply(\n",
    "            lambda row: list(json.loads(row).values())\n",
    "        ),\n",
    "        correctAnswers=lambda x: x[\"correctAnswers\"].apply(\n",
    "            lambda row: list(json.loads(row).values())\n",
    "        ),\n",
    "        annotation_counts=lambda x: x[\"annotations\"].apply(\n",
    "            lambda row: Counter(json.loads(row).values())\n",
    "        ),\n",
    "        annotations=lambda x: x[\"annotations\"].apply(\n",
    "            lambda row: list(json.loads(row).values())\n",
    "        ),\n",
    "    )\n",
    "    # Calculate percentage correct\n",
    "    .assign(\n",
    "        pct_correct=lambda x: x.apply(\n",
    "            lambda row: sum(\n",
    "                a == c for a, c in zip(row[\"answers\"], row[\"correctAnswers\"])\n",
    "            )\n",
    "            / len(row[\"answers\"])\n",
    "            * 100,\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "cloze_df = pd.read_json(\n",
    "    \"../results/cloze_exercises_kl_divergence.jsonl\", lines=True\n",
    ").assign(passageId=lambda x: x.index + 1)\n",
    "\n",
    "df = pd.merge(cloze_df, annotations_df, on=\"passageId\").query(\n",
    "    'method == \"contextuality_plus\"'\n",
    ")\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9949fa8a-d32a-461f-82cf-2111ff827313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'The necessity of a scientific approach in psychology is often questioned, with many relying on common sense or intuition—known as folk psychology—for understanding human behavior. However, scientific research frequently contradicts these _________ beliefs, revealing inaccuracies. For instance, the belief that expressing anger can _________ it has been debunked, as has the notion that false ___________ are rare. Common myths, such as using only 10% of our brain or the effectiveness of _______-reducing diets, persist due to heuristics and confirmation bias. Psychologists emphasize __________ and the pursuit of empirical evidence to _________ these misconceptions. Additionally, they embrace ___________, welcoming unanswered questions as opportunities for scientific ___________.', 'gaps': [['intuitive', 238, 9], ['alleviate', 332, 9], ['confessions', 393, 11], ['calorie', 489, 7], ['skepticism', 586, 10], ['challenge', 638, 9], ['uncertainty', 697, 11], ['exploration', 773, 11]]}\n"
     ]
    }
   ],
   "source": [
    "print(df.contextuality_plus.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164acfc-8983-44ba-b579-063f95027fba",
   "metadata": {},
   "source": [
    "# Test Restricted Generation\n",
    "\n",
    "Only choose gaps whose lemmatized form appears in the source text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1ba1b20-275f-462a-8e0a-42e87fb97ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualityGapper:\n",
    "    def __init__(self, model_name: str = \"answerdotai/ModernBERT-large\"):\n",
    "        # Load SpaCy for sentence splitting and preprocessing\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.min_blank_distance = 7  # Minimum distance between blanks\n",
    "\n",
    "        # Minimum log-predictability of alternatives\n",
    "        self.min_predictability = np.log(0.05)\n",
    "\n",
    "        # Part-of-Speech Blacklist (do not delete these words)\n",
    "        self.blacklist = [\n",
    "            \"PROPN\",  # Proper nouns\n",
    "            \"NUM\",  # Numbers\n",
    "            \"PUNCT\",  # Punctuation\n",
    "            \"SYM\",  # Symbols\n",
    "            \"X\",  # Other\n",
    "        ]\n",
    "\n",
    "    def _get_leading_ws_tokens(self, doc: Doc) -> list[str]:\n",
    "        \"\"\"The ModernBERT Tokenizer will work fine if we give it tokens with leading spaces.\n",
    "        SpaCy normally handles whitespace in terms of trailing space.\"\"\"\n",
    "        if not len(doc):\n",
    "            return [\"\"]\n",
    "\n",
    "        tokens = [doc[0].text]\n",
    "        # For tokens after the 0th, prepend trailing whitespace from the previous token.\n",
    "        tokens += [doc[i - 1].whitespace_ + doc[i].text for i in range(1, len(doc))]\n",
    "        return tokens\n",
    "\n",
    "    def get_token_mappings(self, tokens: list[str]) -> dict[int, list[int]]:\n",
    "        \"\"\"Get mappings between word positions and token positions\"\"\"\n",
    "        # Tokenize while keeping track of word IDs\n",
    "        tokenized = self.tokenizer(\n",
    "            tokens, return_tensors=\"pt\", is_split_into_words=True\n",
    "        )\n",
    "        word_ids = tokenized.word_ids()\n",
    "\n",
    "        # Create mapping from word position to token positions\n",
    "        word_to_tokens = defaultdict(list)\n",
    "\n",
    "        for token_idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx is not None:\n",
    "                word_to_tokens[word_idx].append(token_idx)\n",
    "\n",
    "        return word_to_tokens\n",
    "\n",
    "    def get_masked_logits(\n",
    "        self, tokens: list[str], mask_idx: int\n",
    "    ) -> tuple[torch.Tensor, int]:\n",
    "        \"\"\"Get model logits for a masked position in text\"\"\"\n",
    "        # Get the word tokens and their alignment info\n",
    "        word_to_tokens = self.get_token_mappings(tokens)\n",
    "\n",
    "        # Find all token positions for the word we want to mask\n",
    "        token_positions = word_to_tokens[mask_idx]\n",
    "\n",
    "        # Create masked version of the text\n",
    "        input_ids = self.tokenizer(\n",
    "            tokens, is_split_into_words=True, return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "        masked_ids = input_ids.clone()\n",
    "\n",
    "        # ID of the first subword token that we masked\n",
    "        first_token_id = input_ids[token_positions[0]]\n",
    "\n",
    "        # Mask all tokens corresponding to our target word\n",
    "        masked_ids[token_positions] = self.tokenizer.mask_token_id\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = self.model(input_ids.unsqueeze(0).to(self.device))\n",
    "\n",
    "        # Get logits\n",
    "        logits = outputs.logits[0, token_positions, :]\n",
    "\n",
    "        return logits, first_token_id\n",
    "\n",
    "    def get_contextuality_score(\n",
    "        self,\n",
    "        page_doc: Doc,\n",
    "        summary_doc: Doc,\n",
    "        sent: Span,\n",
    "        tok: Token,\n",
    "        method: str = \"kl\",\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate contextuality score for a word position using full page context\n",
    "\n",
    "        Args:\n",
    "            page_doc: The full page text as a spaCy Doc\n",
    "            summary_doc: The summary text as a spaCy Doc\n",
    "            sent: The sentence from the summary containing the token\n",
    "            tok: The token from the summary to evaluate\n",
    "            method: \"kl\" for kl-divergence or \"contextuality\" for contextuality score\n",
    "\n",
    "        Returns:\n",
    "            Contextuality score\n",
    "        \"\"\"\n",
    "\n",
    "        # Get logits for both full text and sentence text\n",
    "        # For the full text context, we use the page + summary\n",
    "        full_toks = self._get_leading_ws_tokens(page_doc) + self._get_leading_ws_tokens(\n",
    "            summary_doc\n",
    "        )\n",
    "        full_pos = len(page_doc) + tok.i  # Position of token in full document\n",
    "        full_logits, word_id = self.get_masked_logits(full_toks, full_pos)\n",
    "\n",
    "        # For the local context, we use just the sentence from the summary\n",
    "        sent_pos = tok.i - sent.start  # Position of token in the sentence\n",
    "        sent_logits, _ = self.get_masked_logits([tok.text for tok in sent], sent_pos)\n",
    "\n",
    "        # Calculate probabilities using first sub-word token\n",
    "        full_probs = torch.softmax(full_logits[0], dim=0)\n",
    "        sent_probs = torch.softmax(sent_logits[0], dim=0)\n",
    "\n",
    "        p = full_probs[word_id]\n",
    "        q = sent_probs[word_id]\n",
    "\n",
    "        if method == \"kl\":\n",
    "            # KL-divergence is p*log(p/q)\n",
    "            score = float(p * torch.log2(p / q))\n",
    "        elif method == \"contextuality\":\n",
    "            # Contextuality is distance between full-text and sentence probability\n",
    "            score = float(p - q)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown method.\")\n",
    "\n",
    "        return score\n",
    "\n",
    "    def choose_blank_positions(\n",
    "        self, page_doc: Doc, summary_doc: Doc, num_blanks: int\n",
    "    ) -> list[int]:\n",
    "        \"\"\"Choose positions to blank in the summary based on contextuality scores with full page\"\"\"\n",
    "        scores = []\n",
    "        valid_positions = []\n",
    "\n",
    "        page_lemmas = {tok.lemma_ for tok in page_doc}\n",
    "\n",
    "        # Calculate scores for each position in the summary\n",
    "        for i, sent in enumerate(summary_doc.sents):\n",
    "            if i == 0:\n",
    "                continue  # Skip first sentence\n",
    "            for tok in sent:\n",
    "                if (\n",
    "                    len(tok.text) < 3\n",
    "                    or tok.pos_ in self.blacklist\n",
    "                    or tok.is_stop\n",
    "                    or not tok.text.isalpha()\n",
    "                    or tok.lemma_ not in page_lemmas\n",
    "                ):\n",
    "                    scores.append(-float(\"inf\"))\n",
    "                else:\n",
    "                    # Calculate contextuality using both the full page and summary\n",
    "                    score = self.get_contextuality_score(\n",
    "                        page_doc, summary_doc, sent, tok\n",
    "                    )\n",
    "                    scores.append(score)\n",
    "                valid_positions.append(tok.i)\n",
    "\n",
    "        # Convert to numpy for easier manipulation\n",
    "        scores = np.array(scores)\n",
    "\n",
    "        # Choose positions greedily while maintaining minimum distance\n",
    "        positions = []\n",
    "        for _ in range(num_blanks):\n",
    "            if np.all(scores == -float(\"inf\")):\n",
    "                break\n",
    "\n",
    "            # Choose highest scoring position\n",
    "            idx = np.argmax(scores)\n",
    "            pos = valid_positions[idx]\n",
    "            positions.append(pos)\n",
    "\n",
    "            # Zero out scores within minimum distance\n",
    "            start = max(0, idx - self.min_blank_distance)\n",
    "            end = min(len(scores), idx + self.min_blank_distance + 1)\n",
    "            scores[start:end] = -float(\"inf\")\n",
    "\n",
    "        return sorted(positions)\n",
    "\n",
    "    def get_alternates(self, tokens: list[str], topk=5) -> list[dict]:\n",
    "        \"\"\"Get top k predictions for the masked positions in tokens\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries, one per masked position, with candidate words and their probabilities\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        # Find all mask positions\n",
    "        mask_positions = [i for i, token in enumerate(tokens) if token == \"[MASK]\"]\n",
    "\n",
    "        for mask_pos in mask_positions:\n",
    "            word_candidates = {}\n",
    "\n",
    "            # Try different mask lengths (1, 2, or 3 tokens)\n",
    "            for mask_length in range(1, 4):\n",
    "                # Replace the single mask with multiple if needed\n",
    "                masked_tokens = (\n",
    "                    tokens[:mask_pos]\n",
    "                    + [\"[MASK]\"] * mask_length\n",
    "                    + tokens[mask_pos + 1 :]\n",
    "                )\n",
    "\n",
    "                # Get initial predictions for first token\n",
    "                current_candidates = []\n",
    "                logits, _ = self.get_masked_logits(masked_tokens, mask_pos)\n",
    "                probs = torch.softmax(logits[0], dim=0)\n",
    "                top_values, top_indices = torch.topk(probs, topk)\n",
    "\n",
    "                # Start with first token candidates\n",
    "                for idx, prob in zip(top_indices.tolist(), top_values.tolist()):\n",
    "                    current_candidates.append(([idx], prob))\n",
    "\n",
    "                # Build up multi-token predictions if needed\n",
    "                for token_idx in range(1, mask_length):\n",
    "                    new_candidates = []\n",
    "                    for token_ids, prob in current_candidates:\n",
    "                        # Fill in what we've predicted so far\n",
    "                        partial_filled = tokens.copy()\n",
    "                        filled_text = self.tokenizer.decode(token_ids)\n",
    "                        remaining_masks = mask_length - token_idx\n",
    "\n",
    "                        partial_filled = (\n",
    "                            tokens[:mask_pos]\n",
    "                            + [filled_text]\n",
    "                            + [\"[MASK]\"] * remaining_masks\n",
    "                            + tokens[mask_pos + 1 :]\n",
    "                        )\n",
    "\n",
    "                        # Get prediction for next position\n",
    "                        next_logits, _ = self.get_masked_logits(\n",
    "                            partial_filled, mask_pos + 1\n",
    "                        )\n",
    "                        next_probs = torch.softmax(next_logits[0], dim=0)\n",
    "                        next_values, next_indices = torch.topk(next_probs, 1)\n",
    "\n",
    "                        # Add to candidates\n",
    "                        new_token_ids = token_ids + [next_indices[0].item()]\n",
    "                        new_prob = prob * next_values[0].item()\n",
    "                        new_candidates.append((new_token_ids, new_prob))\n",
    "\n",
    "                    current_candidates = new_candidates\n",
    "\n",
    "                # Add final decoded words\n",
    "                for token_ids, prob in current_candidates:\n",
    "                    word = self.tokenizer.decode(token_ids).strip()\n",
    "                    if \" \" in word:\n",
    "                        # Word contains a space (is actually multiple words)\n",
    "                        continue\n",
    "                    if word not in word_candidates or prob > word_candidates[word]:\n",
    "                        word_candidates[word] = prob\n",
    "\n",
    "            # Sort candidates by probability\n",
    "            sorted_candidates = sorted(\n",
    "                word_candidates.items(), key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "            predictions.append({word: prob for word, prob in sorted_candidates[:topk]})\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def generate_cloze(\n",
    "        self,\n",
    "        summary_text: str,\n",
    "        page_text: str = \"\",\n",
    "        num_blanks: int = 10,\n",
    "    ) -> tuple[str, list[str], list[dict[str, float]]]:\n",
    "        \"\"\"Generate a cloze text from summary using page for context\n",
    "\n",
    "        Args:\n",
    "            page_text: The full page text\n",
    "            summary_text: The summary text to create gaps in\n",
    "            num_blanks: Number of blanks to create\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (cloze_text, answers, alternates)\n",
    "        \"\"\"\n",
    "        # Process both texts\n",
    "        page_doc = self.nlp(page_text)\n",
    "        summary_doc = self.nlp(summary_text)\n",
    "\n",
    "        # Choose positions to blank in the summary\n",
    "        masked_positions = self.choose_blank_positions(\n",
    "            page_doc, summary_doc, num_blanks\n",
    "        )\n",
    "\n",
    "        # Get the answers (the original words that will be blanked)\n",
    "        answers = [summary_doc[pos].text for pos in masked_positions]\n",
    "\n",
    "        # Replace tokens with mask\n",
    "        summary_tokens = np.array(self._get_leading_ws_tokens(summary_doc))\n",
    "        summary_tokens[masked_positions] = \"[MASK]\"\n",
    "        summary_tokens = summary_tokens.tolist()\n",
    "\n",
    "        # Construct cloze token input for gap predictions\n",
    "        cloze_tokens = self._get_leading_ws_tokens(page_doc) + summary_tokens\n",
    "\n",
    "        # Collect gaps\n",
    "        gaps = []\n",
    "        for tok in summary_doc:\n",
    "            if tok.i in masked_positions:\n",
    "                gaps.append((tok.text, tok.idx, len(tok.text)))\n",
    "\n",
    "        return gaps\n",
    "\n",
    "    def score_cloze_answers(self, text_with_masks: str, top_k: int) -> list[dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Return, for each [MASK], a dict {token -> probability}.\n",
    "        \"\"\"\n",
    "        # normalize top_k \n",
    "        k_norm = _normalize_topk_for_use(top_k)\n",
    "        k = 5000 if k_norm is None else int(k_norm)\n",
    "    \n",
    "        inputs = self.tokenizer(text_with_masks, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        logits = outputs.logits  # [batch, seq, vocab]\n",
    "    \n",
    "        mask_positions = torch.where(inputs[\"input_ids\"][0] == self.tokenizer.mask_token_id)[0]\n",
    "    \n",
    "        results = []\n",
    "        for pos in mask_positions:\n",
    "            vec = logits[0, pos.item()]            # [vocab]\n",
    "            probs = torch.softmax(vec, dim=-1)\n",
    "            top_probs, top_idx = torch.topk(probs, k)   # <- k is guaranteed int\n",
    "            preds = {}\n",
    "            for p, idx in zip(top_probs.tolist(), top_idx.tolist()):\n",
    "                token = self.tokenizer.decode([idx]).strip()\n",
    "                preds[token] = p\n",
    "            results.append(preds)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f41dc0-e268-407a-9ec0-6505284385cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e76c535a-accd-44d7-97af-52dcbe35499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LH: Use doc strings for function descriptions\n",
    "# LH: The underscore before a function name indicates that it is for internal use only, usually methods in a class that shouldn't be called from outside that class.\n",
    "# LH: I wouldn't use it for functions inside the global scope of a notebook.\n",
    "\n",
    "\n",
    "def _mask_by_char_spans(\n",
    "    text: str, spans: list[tuple[int, int]]\n",
    ") -> str:  # converting character positions from df to masks\n",
    "    parts, prev = [], 0\n",
    "    for start, end in spans:\n",
    "        parts.append(text[prev:start])\n",
    "        parts.append(\"[MASK]\")\n",
    "        prev = end\n",
    "    parts.append(text[prev:])\n",
    "    return \"\".join(parts)\n",
    "\n",
    "def _normalize_topk_for_use(x):\n",
    "    \"\"\"Return an int topk or None (== unlimited). Accepts int/float/NaN/None/'unlimited'.\"\"\"\n",
    "    if x is None: \n",
    "        return None\n",
    "    if isinstance(x, str) and x.strip().lower() == \"unlimited\":\n",
    "        return None\n",
    "    try:\n",
    "        # treat NaN like unlimited\n",
    "        if isinstance(x, float) and np.isnan(x):\n",
    "            return None\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return None  # safest fallback: unlimited\n",
    "\n",
    "def _display_topk(x):\n",
    "    \"\"\"Return a display value for topk: int or 'unlimited'.\"\"\"\n",
    "    k = _normalize_topk_for_use(x)\n",
    "    return \"unlimited\" if k is None else int(k)\n",
    "\n",
    "\n",
    "\n",
    "# LH: Could do this a bit more cleanly like\n",
    "def mask_by_char_spans(text: str, spans: list[tuple[int, int]]) -> str:\n",
    "    \"\"\"Replace text at gapped locations with [MASK] using (start, end) tuples\"\"\"\n",
    "    for start, end in sorted(spans, reverse=True):\n",
    "        text = text[:start] + \"[MASK]\" + text[end:]\n",
    "    return text\n",
    "\n",
    "\n",
    "def _extract_from_contextuality_plus(\n",
    "    row,\n",
    "):  # extract all relevant information from dataframe\n",
    "    \"\"\"\n",
    "    Return:\n",
    "      summary_text : str\n",
    "      gap_spans    : list[(start, end)]\n",
    "      originals    : list[str]\n",
    "    \"\"\"\n",
    "    cp = row.contextuality_plus\n",
    "    summary_text = cp.get(\"text\", row.summary)\n",
    "    gap_spans, originals = [], []\n",
    "    for word, start, length in cp.get(\"gaps\", []):\n",
    "        originals.append(word)\n",
    "        gap_spans.append((start, start + length))\n",
    "    return summary_text, gap_spans, originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7ed305c-cb3f-4f62-ad0b-ca6b0d48dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_accepted_answers(\n",
    "    gapper,\n",
    "    page_text: str,\n",
    "    row,\n",
    "    min_probability: float = 0.05,\n",
    "    topk: int | None = None,\n",
    "):\n",
    "    summary_text, gap_spans, originals = _extract_from_contextuality_plus(row)\n",
    "    masked_summary = mask_by_char_spans(summary_text, gap_spans)\n",
    "\n",
    "    page_str = \"\".join(gapper._get_leading_ws_tokens(gapper.nlp(page_text)))\n",
    "    full_context = page_str + \"\\n\\n\" + masked_summary\n",
    "\n",
    "    k_norm = _normalize_topk_for_use(topk)\n",
    "    k_arg = 5000 if k_norm is None else int(k_norm)\n",
    "\n",
    "    preds_per_gap = gapper.score_cloze_answers(full_context, top_k=k_arg)\n",
    "\n",
    "    accepted = []\n",
    "    for i, pred_dict in enumerate(preds_per_gap):\n",
    "        keep = set()\n",
    "        for w, p in pred_dict.items():\n",
    "            w2 = w.strip()\n",
    "            if w2 and \" \" not in w2 and w2.isalpha() and p >= min_probability:\n",
    "                keep.add(w2.lower())\n",
    "        if i < len(originals):\n",
    "            keep.add(originals[i].strip().lower())\n",
    "        accepted.append(keep)\n",
    "\n",
    "    return accepted, originals\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate accuracy of annotations given new acceptable answers\n",
    "def rescore_annotations(\n",
    "    df,\n",
    "    gapper,\n",
    "    min_probability: float = 0.05,\n",
    "    topk: int | None = None,\n",
    "):\n",
    "    # LH: Put imports at the top of the notebook.\n",
    "    # These are already imported...\n",
    "    import pandas as pd\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    rows = []\n",
    "    for row in tqdm(df.itertuples(), total=len(df), desc=\"Rescoring\"):\n",
    "        accepted, originals = generate_accepted_answers(\n",
    "            gapper=gapper,  # LH: You wouldn't need to pass this class instance around if this function were a class method.\n",
    "            page_text=row.text,\n",
    "            row=row,\n",
    "            min_probability=min_probability,\n",
    "            topk=topk,\n",
    "        )\n",
    "\n",
    "        human = [annotation.strip().lower() for annotation in row.annotations]\n",
    "        flags = []\n",
    "        for i, ans in enumerate(human):\n",
    "            okset = accepted[i] if i < len(accepted) else set()\n",
    "            if not okset and i < len(originals):  # safety fallback\n",
    "                okset = {originals[i].strip().lower()}\n",
    "            flags.append(ans in okset)\n",
    "\n",
    "        percent = 100.0 * (sum(flags) / max(1, len(flags)))\n",
    "        rows.append(\n",
    "            {\n",
    "                \"accepted_answers\": accepted,\n",
    "                \"human_correctness\": flags,\n",
    "                \"percent_correct_alt\": percent,\n",
    "                \"alt_counts\": [len(s) for s in accepted],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    extra = pd.DataFrame(rows, index=df.index)\n",
    "    return pd.concat([df.copy(), extra], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "676a9a70-0232-4e58-aa1a-013c96734f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LH: see above\n",
    "# !pip install -U \"transformers>=4.46\" \"tokenizers>=0.15\" safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e011595b-5546-4f7a-9621-5d97a9b20fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gapper = ContextualityGapper(model_name=\"answerdotai/ModernBERT-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c89ec7b5-243c-4ddc-bece-59064296d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "#      ANALYZE HUMAN AGREEMENT PATTERNS\n",
    "# ============================================\n",
    "def analyze_human_agreement(df):\n",
    "    \"\"\"\n",
    "    Find cases where multiple humans gave the same *incorrect* answer\n",
    "    for the same passage and the same gap index.\n",
    "    Returns: (counts_df, common_df)\n",
    "    \"\"\"\n",
    "\n",
    "    recs = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        pid = row[\"passageId\"]\n",
    "        anns = row[\"annotations\"]\n",
    "        gold = row[\"correctAnswers\"]\n",
    "        # LH: Please use descriptive variable names\n",
    "        for g, (h, c) in enumerate(zip(anns, gold)):\n",
    "            h = h.strip().lower()\n",
    "            c = c.strip().lower()\n",
    "            if h != c:\n",
    "                recs.append({\"passageId\": pid, \"gap_idx\": g, \"human_answer\": h})\n",
    "\n",
    "    if not recs:\n",
    "        print(\"No wrong answers found.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    tmp = pd.DataFrame(recs)\n",
    "    counts = (\n",
    "        tmp.value_counts([\"passageId\", \"gap_idx\", \"human_answer\"])\n",
    "        .reset_index(name=\"n\")\n",
    "        .sort_values(\"n\", ascending=False)\n",
    "    )\n",
    "\n",
    "    common = counts[counts[\"n\"] > 1]\n",
    "\n",
    "    print(\n",
    "        f\"Found {len(common)} (passageId, gap_idx) cases with agreement on the same wrong answer.\"\n",
    "    )\n",
    "    if not common.empty:\n",
    "        print(\"\\nTop 10:\")\n",
    "        print(common.head(10).to_string(index=False))\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo per-gap agreements detected. This usually means you have one annotator per passage, \"\n",
    "            \"or the wrong answers are all unique per gap.\"\n",
    "        )\n",
    "\n",
    "    return counts, common\n",
    "\n",
    "\n",
    "# ============================================\n",
    "#            PARAMETER GRID SEARCH\n",
    "# ============================================\n",
    "\n",
    "\n",
    "def test_parameters(df, gapper, min_prob_values, topk_values=None):\n",
    "    results = []\n",
    "\n",
    "    if topk_values is None:\n",
    "        topk_values = [10_000]  # Effectively unlimited\n",
    "\n",
    "    total_tests = len(min_prob_values) * len(topk_values)\n",
    "    pbar = tqdm(total=total_tests, desc=\"Testing parameters\")\n",
    "\n",
    "    for min_prob in min_prob_values:\n",
    "        for topk in topk_values:\n",
    "            # Normalize for use (None => unlimited)\n",
    "            k_use = _normalize_topk_for_use(topk)\n",
    "\n",
    "            df_rescored = rescore_annotations(\n",
    "                df.copy(),\n",
    "                gapper,\n",
    "                min_probability=min_prob,\n",
    "                topk=k_use,  # pass None or int\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"min_prob\": min_prob,\n",
    "                # -------- FIX: store a *display* topk that is not NaN --------\n",
    "                \"topk\": _display_topk(topk),\n",
    "                # ----------------------------------------------------------------\n",
    "                \"mean_accuracy\": df_rescored[\"percent_correct_alt\"].mean(),\n",
    "                \"median_accuracy\": df_rescored[\"percent_correct_alt\"].median(),\n",
    "                \"std_accuracy\": df_rescored[\"percent_correct_alt\"].std(),\n",
    "                \"mean_alternatives\": df_rescored[\"alt_counts\"].apply(lambda x: np.mean(x)).mean(),\n",
    "                \"median_alternatives\": df_rescored[\"alt_counts\"].apply(lambda x: np.median(x)).median(),\n",
    "                \"max_alternatives\": df_rescored[\"alt_counts\"].apply(lambda x: np.max(x)).max(),\n",
    "                \"gaps_with_1_5_alts\": (\n",
    "                    sum(df_rescored[\"alt_counts\"].apply(lambda x: all(1 <= c <= 5 for c in x)))\n",
    "                    / len(df_rescored) * 100\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            results.append(metrics)\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "#                 BEST PARAMS\n",
    "# ============================================\n",
    "\n",
    "\n",
    "def analyze_best_parameters(df, gapper, min_prob, topk, common_df):\n",
    "    \"\"\"\n",
    "    Analysis of a specific parameter combo.\n",
    "    common_df is the DataFrame returned by analyze_human_agreement(...)[1]\n",
    "    \"\"\"\n",
    "\n",
    "    df_rescored = rescore_annotations(\n",
    "        df.copy(), gapper, min_probability=min_prob, topk=topk\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\n",
    "        f\"DETAILED ANALYSIS: min_prob={min_prob}, topk={topk if topk else 'unlimited'}\"\n",
    "    )\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Overall stats\n",
    "    print(f\"\\nOverall Performance:\")\n",
    "    print(f\"  Mean accuracy: {df_rescored['percent_correct_alt'].mean():.1f}%\")\n",
    "    print(f\"  Median accuracy: {df_rescored['percent_correct_alt'].median():.1f}%\")\n",
    "    print(f\"  Std deviation: {df_rescored['percent_correct_alt'].std():.1f}%\")\n",
    "\n",
    "    # Alternative counts (flatten)\n",
    "    alt_counts_flat = [c for row in df_rescored[\"alt_counts\"] for c in row]\n",
    "    print(f\"\\nAlternative Answers per Gap:\")\n",
    "    print(f\"  Mean: {np.mean(alt_counts_flat):.1f}\")\n",
    "    print(f\"  Median: {np.median(alt_counts_flat):.0f}\")\n",
    "    print(f\"  Max: {np.max(alt_counts_flat)}\")\n",
    "    print(\n",
    "        f\"  % gaps with 1-5 alternatives: {sum(1 <= c <= 5 for c in alt_counts_flat) / len(alt_counts_flat) * 100:.1f}%\"\n",
    "    )\n",
    "\n",
    "    # How many “common wrong” are now accepted for the same passage+gap\n",
    "    accepted_mistakes = 0\n",
    "    total_mistakes = len(common_df)\n",
    "\n",
    "    if total_mistakes:\n",
    "        # Build a quick lookup: passageId -> row in rescored df\n",
    "        rescored_by_pid = df_rescored.set_index(\"passageId\")\n",
    "\n",
    "        for _, rec in common_df.iterrows():\n",
    "            pid = rec[\"passageId\"]\n",
    "            gap = int(rec[\"gap_idx\"])\n",
    "            ans = str(rec[\"human_answer\"]).strip().lower()\n",
    "\n",
    "            if pid in rescored_by_pid.index:\n",
    "                row = rescored_by_pid.loc[pid]\n",
    "                if gap < len(row[\"accepted_answers\"]):\n",
    "                    if ans in row[\"accepted_answers\"][gap]:\n",
    "                        accepted_mistakes += 1\n",
    "\n",
    "    print(f\"\\nCommon Human Agreements (per passage+gap):\")\n",
    "    if total_mistakes:\n",
    "        print(\n",
    "            f\"  Accepting {accepted_mistakes}/{total_mistakes} agreed-on 'incorrect' answers\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"  (None found in this dataset)\")\n",
    "\n",
    "    return df_rescored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f20ec33-5f78-455f-bd56-a26b8ac6e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_testing(df, gapper):\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PARAMETER OPTIMIZATION FOR CLOZE SCORING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nSTEP 1: Analyzing Human Agreement Patterns_____\")\n",
    "    counts_df, common_df = analyze_human_agreement(df)\n",
    "\n",
    "    print(\"\\nSTEP 2: Testing Parameter Combinations_____\")\n",
    "    min_prob_values = [0.01, 0.02, 0.03, 0.05, 0.07, 0.10, 0.15, 0.20]\n",
    "    topk_values = [5, 10, 20, 50, None]  # None = unlimited\n",
    "    results_df = test_parameters(df, gapper, min_prob_values, topk_values)\n",
    "\n",
    "    print(\"\\nSTEP 3: Finding Optimal Parameters_____\")\n",
    "    results_df[\"score\"] = (\n",
    "        results_df[\"mean_accuracy\"] * 2\n",
    "        + results_df[\"gaps_with_1_5_alts\"]\n",
    "        + (5 - abs(results_df[\"mean_alternatives\"] - 3)) * 10\n",
    "    )\n",
    "    best_params = results_df.nlargest(5, \"score\")\n",
    "    print(\"\\nTOP 5 PARAMETER COMBINATIONS:\")\n",
    "    print(\n",
    "        best_params[\n",
    "            [\"min_prob\", \"topk\", \"mean_accuracy\", \"mean_alternatives\", \"gaps_with_1_5_alts\", \"score\"]\n",
    "        ].to_string(index=False)\n",
    "    )\n",
    "\n",
    "    best_row = best_params.iloc[0]\n",
    "    best_min_prob = float(best_row[\"min_prob\"])\n",
    "\n",
    "    # convert display topk back to usable value --------\n",
    "    best_topk = _normalize_topk_for_use(best_row[\"topk\"])  # None or int\n",
    "\n",
    "    print(\"\\nSTEP 4: Best Parameters______\")\n",
    "    df_best = analyze_best_parameters(df, gapper, best_min_prob, best_topk, common_df)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nRECOMMENDED PARAMETERS:\")\n",
    "    print(f\"  min_probability = {best_min_prob}\")\n",
    "    print(f\"  topk = {_display_topk(best_topk)}\")\n",
    "    print(f\"\\nExpected Performance:\")\n",
    "    print(f\"  Mean human accuracy: {best_row['mean_accuracy']:.1f}%\")\n",
    "    print(f\"  Mean alternatives per gap: {best_row['mean_alternatives']:.1f}\")\n",
    "    print(f\"  Gaps with 1-5 alternatives: {best_row['gaps_with_1_5_alts']:.1f}%\")\n",
    "\n",
    "    return results_df, df_best, best_min_prob, best_topk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c4cd1-0702-43f6-a98c-aa532def3736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PARAMETER OPTIMIZATION FOR CLOZE SCORING\n",
      "============================================================\n",
      "\n",
      "STEP 1: Analyzing Human Agreement Patterns_____\n",
      "Found 41 (passageId, gap_idx) cases with agreement on the same wrong answer.\n",
      "\n",
      "Top 10:\n",
      " passageId  gap_idx  human_answer  n\n",
      "         5        1      sentence  7\n",
      "         5        6        source  6\n",
      "         5        8        source  6\n",
      "         5        2      sentence  6\n",
      "         5        4      sentence  6\n",
      "         5        3      sentence  5\n",
      "         5        5      sentence  5\n",
      "         1        6        source  4\n",
      "         5        7      sentence  4\n",
      "         1        7 unpredictable  4\n",
      "\n",
      "STEP 2: Testing Parameter Combinations_____\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169a959f5f7c442b8bd3495b9cf0ed97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing parameters:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b79c406d8842fea596fdde37e54dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c314136f39f4866b74392023948cb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342fbab9a5c84ef9b8ca80d51f861d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf629e3614d0438f9735342df1f5eed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccd9787831943b7802b5c33e6774d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff2318251ce4833991b66976ffed416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169188773e484deb8b718120085cfc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b740c6e5a34b59b24e4814119fc554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71dcda5042344b396a1b27b6dff9be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f70b82655c649d0b3e5c754128a2b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef737b3201e4361b8877752a5505a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e0c18c2b86456eaea66a3e93f56e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5a8c1ffccb4061910725a91fb62e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f388c145db0842eda254f7c8719c59d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03e3b426f254ebaa733ea755362cd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2301871aae742ce84859e452c566741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3e990d0d8042a5b6869aa4e759fcf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7deee067d5a5429fa1a5c52fcd95cd6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb21e24a8f8f408eaf7d977c9d1bde1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a981b842c81480380b374e6739ca45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c1f6c8fcde41f785d332c4c51d6a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fe469c0c4645c6b9fece2e7fc9c7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7691361b91c547f7a0e513176c56a86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1287d62f334493803ac7657a347493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac5fc376d08499eb873819e991758f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e2726556e84d31ad4ee9531d8f5515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63615c40cc8f4963845869587fc65f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccb6b146eb642b1b10d93281ccf4e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6a78ece97f4c6cab4dca8fbcb34fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da045e70a3b494db6e07c9943d20538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb407f110654af8a3ac04f3275ed407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe605f38cb743529400987e567758c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6156371c7fb74c268dc3ba32bdf73f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aeee719e9f242029709798ab18f05dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaff29d9ec9244fcac6cb1213855b837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c589e52a69a4d108a8455a95e22572c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a8a10ce72847de963259ec5042aec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rescoring:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df, df_best, best_min_prob, best_topk = run_testing(df, gapper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807ab4a-9ed0-4bba-bb02-5ce23eb03b2a",
   "metadata": {},
   "source": [
    "## Compare Gaps\n",
    "\n",
    "Without collecting additional annotations, the best we can do is look at which gaps would be retained with this method and which gaps would be removed.\n",
    "\n",
    "If this method disproportionately removes gaps that were more difficult to answer, then we can infer that the lemma overlap restriction will make the cloze exercise easier (which is what we want).\n",
    "\n",
    "First, we see that the lemma overlap constraint does not substantially decrease the number of gaps that are generated (8.65 gaps per exercise vs. 9 gaps per exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e35b07f-3b6a-406c-b053-768aa4f3c4c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'restricted_answers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_99/3970999004.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestricted_answers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrectAnswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'restricted_answers'"
     ]
    }
   ],
   "source": [
    "display(df.restricted_answers.str.len().describe())\n",
    "display(df.correctAnswers.str.len().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d279c1-0770-4e7b-abad-6cc564cb4b35",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The lemma overlap restriction retains 44% of gaps that are \"source-predictable\" (ideal), and removes 31% of gaps that were scored as \"unpredictable\" (bad gaps). This is a good indication that the lemma overlap constraint improves the cloze exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "686c5219-983b-4c0b-94d3-08cfb724482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained gaps\n",
      "[('passage', 0.18), ('sentence', 0.3), ('source', 0.44), ('unpredictable', 0.08)]\n",
      "\n",
      "Removed gaps\n",
      "[('passage', 0.17), ('sentence', 0.43), ('source', 0.09), ('unpredictable', 0.31)]\n"
     ]
    }
   ],
   "source": [
    "unchanged = []\n",
    "removed = []\n",
    "# added = []\n",
    "\n",
    "\n",
    "def normalize_counter(c: Counter):\n",
    "    total = sum(c.values())\n",
    "    for key in c:\n",
    "        c[key] = round(c[key] / total, 2)\n",
    "    return c\n",
    "\n",
    "\n",
    "for row in df.itertuples():\n",
    "    for answer, rating in zip(row.correctAnswers, row.annotations):\n",
    "        if answer in row.restricted_answers:\n",
    "            unchanged.append(rating)\n",
    "        else:\n",
    "            removed.append(rating)\n",
    "\n",
    "print(\"Retained gaps\")\n",
    "print(sorted(normalize_counter(Counter(unchanged)).items()))\n",
    "print(\"\\nRemoved gaps\")\n",
    "print(sorted(normalize_counter(Counter(removed)).items()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
